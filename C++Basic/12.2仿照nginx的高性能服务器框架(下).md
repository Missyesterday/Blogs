# 12.2  仿照Nginx的高性能服务器框架(下)

## 5. 网络通讯理论

### 5.1 客户端和服务器

客户端和服务器都是程序。

连接一般是是客户端发起的，但是连接建立后，数据流动是双向的（双工）。

端口是0～65535之间的一个数，计算机不允许两个程序监听一个端口。浏览器也有一个端口，但是浏览器的端口是随机的，不是固定的。

### 5.2 网络模型

#### 5.2.1 OSI七层网络模型

物链网传会表应，想象成一个人穿了七件衣服。

#### 5.2.2 TPC/IP 协议四层模型

事实上的协议。 传输控制协议/网际协议。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230305141323403.png" alt="image-20230305141323403" style="zoom:40%;" />

TCP/IP四层模型更为简单，它是一组协议。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230305141506424.png" alt="image-20230305141506424" style="zoom:40%;" />

#### 5.2.3 TCP/IP协议解释和比喻

就相当于人上街要穿衣服。



#### 5.2.4 TCP和UDP的区别

使用`socket()`时候指定不同的参数，就能使用不同的协议，后面的参数也需要进行相应修改。

-   可靠VS不可靠

-   连接VS无连接

TCP：

-   耗费更多的系统资源确保数据的可靠，传输的数据一定正确，不丢失，不重复，按顺序叨叨

UDP：

-   发送速度特别快，效率高，不保证数据的可靠性，QQ聊天信息用UDP（内部有一些弥补的机制），DNS解析
-   随着硬件的发展，UDP越来越可靠，随着网络的发展，可能UDP的适用性更广



### 5.3 最简单的C/S通信程序

可以参考《Unix网络编程》第一卷，里面有很多小demo。

演示一下（玩具程序）：

#### 5.3.1 套接字Socket的概念

-   套接字socket就是一个数字，通过调用`socket()`函数来生成，这个数字具有唯一性：可以一直使用，直到调用`close()`函数
-   socket这个数字被视为一个文件描述符，可以利用socket来收发数据（`send()`和`recv()`函数）

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230305145312713.png" alt="image-20230305145312713" style="zoom:50%;" />

#### 5.3.2 一个简单的服务器通信程序

<mark>需要关注调用了哪些函数和调用顺序</mark>

**5_1_1server.c**

属于是八股文，需要注意服务器端有两个socket，一个用于监听，一个用于通信（可能有多个）

```cpp

#include <stdio.h>
#include <ctype.h>
#include <unistd.h>
#include <sys/types.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <stdlib.h>
#include <string.h>

#define SERV_PORT 9000  //本服务器要监听的端口号，一般1024以下的端口很多都是属于周知端口，所以我们一般采用1024之后的数字做端口号

int main(int argc, char *const *argv)
{    
    //这些演示代码的写法都是固定套路，一般都这么写

    //服务器的socket套接字【文件描述符】
    int listenfd = socket(AF_INET, SOCK_STREAM, 0);    //创建服务器的socket，可以暂时不用管这里的参数是什么，知道这个函数大概做什么就行

    struct sockaddr_in serv_addr;                  //服务器的地址结构体
    memset(&serv_addr,0,sizeof(serv_addr));
    
    //设置本服务器要监听的地址和端口，这样客户端才能连接到该地址和端口并发送数据
    serv_addr.sin_family = AF_INET;                //选择协议族为IPV4
    serv_addr.sin_port = htons(SERV_PORT);         //绑定我们自定义的端口号，客户端程序和我们服务器程序通讯时，就要往这个端口连接和传送数据
    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY); //监听本地所有的IP地址；INADDR_ANY表示的是一个服务器上所有的网卡（服务器可能不止一个网卡）多个本地ip地址都进行绑定端口号，进行监听。

    bind(listenfd, (struct sockaddr*)&serv_addr, sizeof(serv_addr));//绑定服务器地址结构体
    listen(listenfd, 32);     //参数2表示服务器可以积压的未处理完的连入请求总个数，客户端来一个未连入的请求，请求数+1，连入请求完成，c/s之间进入正常通讯后，请求数-1

    int connfd;
    const char *pcontent = "I sent sth to client!"; //指向常量字符串区的指针
    while(1)
    {
        //卡在这里，等客户单连接，客户端连入后，该函数走下去【注意这里返回的是一个新的socket——connfd，后续本服务器就用connfd和客户端之间收发数据，而原有的lisenfd依旧用于继续监听其他连接】        
        connfd = accept(listenfd, (struct sockaddr*)NULL, NULL);

        //发送数据包给客户端
        write(connfd, pcontent, strlen(pcontent)); //注意第一个参数是accept返回的connfd套接字
        
        //只给客户端发送一个信息，然后直接关闭套接字连接；
        close(connfd); 
    } //end for
    close(listenfd);     //实际本简单范例走不到这里，这句暂时看起来没啥用
    return 0;
}
```



#### 5.3.3 IP地址简介

绝大部分地址是IPv4地址，也渐渐有了IPv6，写服务器不需要考虑IPv4和IPv6问题，遵照IPv4就行，IPv6的问题交给硬件厂商。

写客户端程序，需要改动，但改动不是很大。



#### 5.3.4 IPv4客户端通信程序

```cpp

#include <stdio.h>
#include <ctype.h>
#include <unistd.h>
#include <sys/types.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <stdlib.h>
#include <string.h>


#define SERV_PORT 9000    //要连接到的服务器端口，服务器必须在这个端口上listen着

int main(int argc, char *const *argv)
{    
    //这些演示代码的写法都是固定套路，一般都这么写
    int sockfd = socket(AF_INET, SOCK_STREAM, 0); //创建客户端的socket

    struct sockaddr_in serv_addr; 
    memset(&serv_addr,0,sizeof(serv_addr));

    //设置要连接到的服务器的信息
    serv_addr.sin_family = AF_INET;                //选择协议族为IPV4
    serv_addr.sin_port = htons(SERV_PORT);         //连接到的服务器端口，服务器监听这个地址
    //这里为了方便演示，要连接的服务器地址固定写
    if(inet_pton(AF_INET,"192.168.1.126",&serv_addr.sin_addr) <= 0)  //IP地址转换函数,把第二个参数对应的ip地址转换第三个参数里边去，固定写法
    {
        printf("调用inet_pton()失败，退出！\n");
        exit(1);
    }

    //连接到服务器
    if(connect(sockfd,(struct sockaddr*)&serv_addr,sizeof(serv_addr)) < 0)
    {
        printf("调用connect()失败，退出！\n");
        exit(1);
    }

    int n;
    char recvline[1000 + 1]; 
    while(( n = read(sockfd,recvline,1000)) > 0) //仅供演示，非商用，所以不检查收到的宽度，实际商业代码，不可以这么写
    {
        recvline[n] = 0; //实际商业代码要判断是否收取完毕等等，所以这个代码只有学习价值，并无商业价值
        printf("收到的内容为：%s\n",recvline);
    }
    close(sockfd); //关闭套接字
    printf("程序执行完毕，退出!\n");
    return 0;
}
```

 

### 5.4 TCP/IP

#### 5.4.1 最大传输单元MTU

MTU：maximum Transfer Unit 最大传输单元，每个数据包最多可以有多少个字节（1.5k左右）

如果超过这个数量，操作系统会分片。一端拆包，一端组包。

#### 5.4.2 TCP包包头结构



#### 5.4.3 TCP数据首发之前的准备工作

三次握手是针对TCP而言的：

-   客户端给服务器发送 SYN置位的无内容（包体）的 数据包
-   服务器收到后 发送一个 SYN和ACK置位的 无内容的 数据包
-   客户端再次发送 ACK置位的数据包

后续就可以进行通信了。



>   为什么三次握手而不是两次握手？
>
>   原因有很多，都是为了确保数据稳定可靠的收发。
>
>   三次握手很大程度上是为了防止恶意破坏TCP连接的验证机制，而不仅仅只是浅显的「确认双方都有收发能力」。
>
>   尽量减少伪造数据包对服务器的攻击，例如伪造一个IP地址和端口发送一个请求，服务端向这个IP地址和端口发送数据（并有随机数），如果不能收到「这个IP地址和端口」的应答（随机数+1），则服务器能发现伪造的IP地址和端口。



#### 5.4.4 telnet工具

这是一个命令行方式运行的客户端TCP通讯工具，可以连接到服务器端，往服务器端发送数据，也可以接受从服务器发送过来的数据。

该工具能非常方便的测试服务器某个「TCP端口」是否能正常收发数据，所以非常实用。



```bash
telnet ip地址 端口
```

中间用空格隔开。

Windows下的telnet敲入一个字符发送一个字符，而Linux按回车才会发送。



#### 5.4.5 wireshark

希望抓去本机和服务器9000端口的包。

在过滤器中输入`host 192.168.31.242 and port 9000`

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230305170146369.png" alt="image-20230305170146369" style="zoom:50%;" />

使用telnet，出现的前三行就是「三次握手」

「四次挥手」：服务器和客户端都有可能是发起断开请求的一方。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230305171330664.png" alt="image-20230305171330664" style="zoom:50%;" />

这是客户端发送了两次ACK，把这两次视为1次就是四次挥手。

#### 5.4.5 TCP状态转换

不能同时开两个监听同一个端口的服务器，只能被`bind()`一次，失败时会显示：

```bash
Address already in use
```



`netstat`命令可以显示网络相关信息，例如端口状态等：

-   -a：显示所有选项
-   -n：能显示成数字的内容全部显示成数字
-   -p：显示端口对应程序名

`netstat -anp | grep -E 'State|9000'`：

-   `0.0.0.0`代表本机的任意地址
-   `State`：
    -   `LISTEN`：正在被监听





<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230305233452243.png" alt="image-20230305233452243" style="zoom:50%;" />

我们如果用两个`telnet`连接服务器，并关闭客户端，可以看到原来的监听端口一直在监听，虽然这两个连接被`close()`了，但是状态信息是`TIME_WAIT`，在出现`TIME_WAIT`状态的时候，服务器的重启会失效（显示`bind返回的值为-1,错误码为:98，错误信息为:Address already in use;`）

只要客户端连接到服务器，并且服务器把客户端关闭，那么服务器端就会产生一条针对9000监听端口的`TIME_WAIT`状态连接。这个状态的连接存在时，kill掉服务器，然后重启就会失败（`bind()`函数返回失败）。



`TIME_WAIT`就是TCP状态转换，《Unix网络编程 第三版 卷1》第二章第六节：2.6.4小节有一个TCP状态转换图。第二章第七节进一步介绍了`TIME_WAIT`。

![](https://static.coonote.com/4010128-b83e0e722007f62a.webp)

需要注意的是，不一定是Client发送断开连接的请求，也有可能是Server，取决于谁调用`close()`。



<img src="https://static.coonote.com/4010128-2556119fac667ac9.webp" style="zoom:50%;" />



TCP状态转换图是针对一个连接而言的。

-   绿色表示服务器
-   红色实线对应客户端



**客户端：**

-   最开始处于`CLOSED`状态，主动发送第一个握手包进入`SYN_SEND`状态
-   收到服务器的`SYN`，进入`ESTABLISHED`
-   收到服务器的`FIN`，进入`CLOSE_WAIT`状态
-   发送`SYN`，进入`LAST_ACK`状态



**服务端：**

-   调用`LISTEN`进入`LISTEN`状态
-   收到`SYN`，发送第二次握手包，进入`SYN_RCVD`状态
-   收到`ACK`，进入`ESTABLISHED`
-   服务器主动关闭连接，发送`FIN`，进入`FIN_WAIT_1`状态
-   服务器收到`ACK`，进入`FIN_WAIT_2`状态
-   服务器收到`FIN`，进入`TIME_WAIT`状态



#### 5.4.6 退出`TIME_WAIT`



具有`TIME_WAIT`状态的TCP连接，就像一种残留信息一样，导致服务器重启需要等待。 在第三次挥手的时候，服务器就进入了`TIME_WAIT`状态，有时间限制，不同操作系统不同（大概1到4分钟）== 2MSL。

MSL：最长的数据包生命周期。



>   为什么引入`TIME_WAIT`状态，并且等待2MSL的原因？
>
>   《Unix网络编程》中说：
>
>   1.   可靠地实现TCP全双工的终止
>   2.    允许老的重复的TCP数据包在网络中消失

如果服务器发送的最后一个`ACK`（应答）丢了，那么客户端一定会重新发送第三次挥手的`FIN`包，因为服务器端有`TIME_WAIT`的存在，那么服务器有机会给客户端再次发送`RST`连接复位包，客户端某些函数就会报错，有这个状态存在，能尽量保证这四个包能正常收发；如果没有`TIME_WAIT`状态，无论客户端是否收到最后的`ACK`包，服务器都已经关闭连接了，此时客户端重新发送`FIN`，服务器返回的就不是`ACK`包，而是`RST`（连接复位）包，从而使客户端没有完成正常的四次挥手，不友好，而且有可能造成数据包丢失。

也就是说，`TIME_WAIT`有助于可靠地实现TCP全双工连接的终止。



#### 5.4.7 `RST`标志

对于每一个TCP连接，OS都要开辟出一个「收缓冲区」和一个发「发缓冲区」来处理数据的收发。

当`close()`一个TCP连接的时候，如果我们发缓冲区有数据，那么OS会「优雅」地把发缓冲区的数据发送完毕，再发送`FIN`包告诉客户端「我要关闭连接了」。

所以说，四次挥手（`FIN`）是一个优雅的关闭标志，表示正常的TCP连接关闭。

反观`RST`标志，就是一个简单粗暴地关闭，出现这个标志的包一般 表示「异常关闭」连接，一般都会导致丢失一些数据包。如果使用`setsockopt()`的`SO_LINGER`选项，抓包就有可能出现`RST`包，此时服务器的发缓冲区中如果有数据，则会被丢弃，导致客户端收不到某些数据。是异常的、粗暴的关闭，不是四次挥手的关闭，如果这么关闭TCP连接，那么主动关闭方也不会进入`TIME_WAIT`状态。



上一个连接的最后一个`ACK`包还没有发送到，结果新的连接（相同的IP和端口）发起新的连接，会收到残留的TCP包，`TIME_WAIT`会丢弃新连接的包。



#### 5.4.8 `SO_REUSEADDR`选项

`setsockopt()`函数一般用在服务端的`socket()`创建之后，`bind()`之前。

其中，`SO_REUSEADDR`可以解决`TIME_WAIT`导致重启失败的问题。

`SO_REUSEADDR`的能力：

-   允许启动一个监听服务器并bind端口，即使以前建立的将该端口用作它们的本地端口的连接仍旧存在。说人话就是，即便`TIME_WAIT`存在，服务器`bind()`也能成功
-   允许同一个端口上端口启动同一个服务器的多个实例（进程），只要每个实例捆绑一个不同的本地IP地址即可（多个网卡可能会有多个IP地址）
-   允许单个进程捆绑同一个端口到多个套接字，只要每次捆绑指定不同的本地IP地址即可
-   允许完全重复地绑定：当一个IP地址和一个端口已经绑定到某个套接字上时，如果传输协议支持，那么同样的IP地址和端口还可以绑定到另一个套接字上；**一般来说，本特性仅支持UDP**。



>   所有TCP服务器都应该指定本套接字选项，以防止当套接字处于`TIME_WAIT`时`bind()`失败的情况。



**使用举例， 在`socket()`后，`bind()`前：**

```cpp

//setsockopt（）:设置一些套接字参数选项；
//参数2：是表示级别，和参数3配套使用，也就是说，参数3如果确定了，参数2就确定了;
//参数3：允许重用本地地址
int  reuseaddr=1; //开启
if(setsockopt(listenfd,SOL_SOCKET,SO_REUSEADDR, (const void *) &reuseaddr,sizeof(reuseaddr)) == -1)
{
    char *perrorinfo = strerror(errno); 
    printf("setsockopt(SO_REUSEADDR)返回值为%d,错误码为:%d，错误信息为:%s;\n",-1,errno,perrorinfo);
}


```

不能重复绑定，但是可以解决`TIME_WAIT`引起的重启问题。



### 5.5 `listen()`队列

`listen()`函数是用来监听端口，用在TCP连接中的服务端。

**`listen()`函数的调用格式：**

```cpp
int listen(int sockfd, int backlog);
```

#### 5.5.1 监听套接字队列

对于一个调用`listen()`进行监听的套接字，操作系统 会给这个套接字维护两个队列：

-   未完成连接队列
-   已完成连接队列

**未完成连接队列：**

-   当客户端发送TCP连接三次握手的第一次（`SYN`数据包），服务器就会在未完成的队列中 创建一个和这个`SYN`对应的一项，可以把这一项看作一个「半连接」（因为连接还没建立起来），这个「半连接」的状态会从`LISTEN`变成`SYN_RCVD`状态，同时操作系统会给客户端返回第二次握手包（`SYN`+`ACK`）。
-   此时服务器在等待「客户端发送 的第三次握手包（`ACK`）

**已完成连接队列：**

-   当客户端发送过来第三次握手包，服务器收到，这个连接就变成了`ESTABLISHED`状态，未完成连接队列的一项就被移动到「已完成连接队列」那么每个已经完成三次握手的客户端都放在这个队列中，作为一项。



<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306151057675.png" alt="image-20230306151057675" style="zoom:50%;" />

`backlog`曾经的含义是「已完成连接队列」+「未完成连接队列」\<\=`backlog`

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306151822113.png" alt="image-20230306151822113" style="zoom:50%;" />

-   <mark>可以看到客户端`connect()`的返回是「收到三次握手的第二个包（服务器的`SYN+ACK`）」</mark>
-   `RTT`是未完成队列中任意一项留在「未完成队列」的时间，这个时间取决于服务端和客户端。往返时间(RTT) 是以毫秒(ms) 为单位的网络请求从起点到达目的地并再次返回起点所需的持续时间。
-   对于客户端和服务器而言，RTT时间是不同的。TCP连接建立大约需要187ms，建立TCP连接的成本是较高的。所以有的网络游戏使用「短连接」。
-   如果一个恶意客户迟迟不发送三次握手的第三个包，那么连接就会建立不起来，服务器就长时会处于`SYN_RCVD`状态，停留时间大约是75秒，超过这个时间，这一项就会被OS干掉。

#### 5.5.2 `accept()`函数

-   `accept()`函数就是用来 「从已完成队列的队首位置」取出来一项（每一项都是一个已经完成三次握手的TCP连接），把这一项返回给进程。
-   如果 「已完成连接队列」是空，没有特殊处理就会阻塞等待，直到「已完成连接队列不为空」。
-   所以，从编程角度来说，要尽快地用`accept()`把已完成连接队列中的数据取走。
-   `accept()`返回的是一个套接字，所以实际上「已完成连接的队列」和「未完成连接的队列」中存储的就是一个套接字。
-   服务器程序必须严格区分：
    -   监听端口的套接字`listenfd`，叫「监听套接字」，只要服务器程序在运行，这个套接字就应该一直存在。
    -   当客户端连接进来，OS会为每个成功建立三次握手的客户端再创建一个套接字，`accept()`返回的就是这个套接字，也就是从「已完成连接队列」中取得的一项。

>   **思考：**
>
>   -   如果两个队列之和「已完成连接队列」+「未完成连接队列」达到了`backlog`，也就是说队列满了，此时再有客户端发送`SYN`请求，服务器会怎么做？
>
>       实际上，服务器会忽略这个`SYN`，不给回应；客户端则会过一会儿会再次发送`SYN`。
>
>   -   从连接被扔到已完成队列中去，到调用`accept()`从已完成连接队列取出，这是有时间差的，如果还没等`accept()`取走，客户端就发来数据，会怎么样？
>
>       这个数据会保存在已经连接套接字的接收缓冲区里，这个缓冲区有多大就能接受多少数据。



#### 5.5.3 `SYN`攻击(SYN flood)

这是一种典型的利用TCP/IP协议设计弱点进行攻击的行为，就是不停给服务器发送第一次TCP握手的`SYN`包，同时IP地址和端口都是伪造的，也不会发送`ACK`包。这样就会导致连接队列比`backlog`大，此时服务器会忽略继续发送的握手包，从而收不到正常的请求。

DoS（*拒绝服务攻击*）和DDos（分布式拒绝服务攻击）都是使用了这个。DDoS就是是用了多台主机，分布式攻击。

所以现在`backlog`不再是两个队列的条目之和，现在的含义是：指定给定套接字上内核为之排队的最大已完成连接数，也就是只定义了一个队列的最大长度。

在实际中，即便如此，SYN攻击还是能把服务器攻击死。所以尽快用`accept()`把已完成队列中的连接取走。经快留出空位给后续已经完成三次握手的条目用，一般`backlog`给300左右即可。

>   DDoS 就是分布式*拒绝服务攻击*，这种网络攻击形式尝试用恶意流量淹没网站或网络资源，从而导致网站或网络资源无法正常运行。 



### 5.6 阻塞与非阻塞I/O

阻塞和非阻塞主要是指调用「系统函数」时，这个函数是否会导致进程进入`sleep()`休眠状态而言的。

#### 5.6.1 阻塞IO

调用一个函数，这个函数由于不满足某些条件，整个程序流程不往下走了（休眠sleep），该函数**卡**在这里等待某个事件发生，只有当这个事件发生了，就认为是一个阻塞函数。

`accept()`可以是一个阻塞函数，也可以是不阻塞的，它的阻塞与否是和「它监听的套接字`listenfd`是否阻塞」有关，而与`accept()`本身无关。

这种阻塞显然并不好，效率很低，从OS的角度来看，OS会给每个进程分配时间片，每个程序都应该尽量把时间片用完。如果进程阻塞在某个地方，OS就会把时间片让给别的进程，一般不会有阻塞的方式来写服务器程序，效率太低了。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306162027265.png" alt="image-20230306162027265" style="zoom:50%;" />

一句话概括：「卡在某个地方，等待数据复制过来」。



#### 5.6.2 非阻塞IO

我们可以通过调用`某些`函数来改变`listenfd`的性质，使得`accept()`为非阻塞，当然这样做`accept()`可能会返回一些错误信息。

这样不会卡住，充分利用时间片，执行效率更高。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306162529075.png" alt="image-20230306162529075" style="zoom:50%;" />

没数据来就会返回其他的值，然后循环调用`recvfrom()`，只要来了数据，`recvfrom()`就会返回成功。

非阻塞的特点：

-   需要不断调用「非阻塞函数」来检查是否有数据到来，如果没有，函数会返回一个特殊的错误标记，这种标记可能是`EWULDBLOCK`或`EAGAIN`，如果数据没到来，这里有机会执行其他函数，但是也得不停地再次调用「非阻塞函数」，所以 程序可能非常 辛苦。
-   如果数据到来，那么「非阻塞函数」就得**卡在这里（阻塞）**把数据从内核缓冲区复制到用户空间



<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306163057958.png" alt="image-20230306163057958" style="zoom:50%;" />

前四种都可以称为「同步IO」[https://www.cnblogs.com/felixzh/p/10345929.html](https://www.cnblogs.com/felixzh/p/10345929.html)



### 5.7 同步和异步I/O

「同步/异步」容易和「阻塞/非阻塞」混淆。

#### 5.7.1 异步I/O

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306163343413.png" alt="image-20230306163343413" style="zoom:50%;" />

当使用异步IO函数的时候，需要提供一个「接收数据的缓冲区」和一个「回调函数」。剩下的事情交给OS。

`aio_read()`是一个异步IO函数，不管有没有数据，异步I/O函数会立即返回，其余判断交给OS。当有数据来的时候，OS会把数据复制到「提供的接收数据的缓冲区」，同时会调用「回调函数」，表示数据来了。

我们可以把这个「回调函数」视为一个数据到来的通知。

>   所以很容易区分「非阻塞IO」和「异步IO」的区别：
>
>   -   「非阻塞IO」要不停地调用IO函数检查数据是否来，如果数据来了，会**阻塞**直到数据从内核缓冲区复制到用户缓冲区，然后这个函数才能返回
>   -   「异步IO」根本不需要不停调用IO函数检查数据是否到来，只需要调用一次，然后程序就可以干别的事情。内核判断数据到来，拷贝数据到用户缓冲区（自己提供的），调用指定的回调函数，不会有任何的阻塞。



#### 5.7.2 同步IO

操作系统的`select/poll`都是同步IO，`epoll`也可以划分到同步IO中来。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306164506833.png" alt="image-20230306164506833" style="zoom:50%;" />

同步IO调用了两个函数，`select()`用来检测是否有数据，`select()`返回之后，用`recvfrom()`去取数据。当然复制数据的时候还是会阻塞。

看起来同步IO更加麻烦，需要调用两个函数才能拿到数据，但是同步IO和阻塞IO比，就是有IO复用能力。

很多资料把 阻塞I/O、非阻塞I/O，I/O复用和信号驱动都称为「同步IO」，这也算是正确的。把异步I/O单独归结成一类，因为它完全没有阻塞的情况。

>   POSIX(可移植操作系统接口)把同步IO操作定义为导致进程阻塞直到IO完成的操作，反之则是异步IO

### 5.8 I/O复用

`select()`的能力就是等待多条TCP连接上的任意一条有数据到来，哪条TCP连接有数据来，再用具体的比如`recvfrom()`去接收数据（只能收一条）。



所谓I/O复用，也就是「多个TCP连接」可以弄成一堆，视为一个整体。使用`select()`这种同步IO函数等待数据。

这种调用一个函数能「判断一堆TCP连接是否来数据」的能力，叫做IO复用（I/O multiplexing），或者叫I/O多路复用。



>   **思考：**
>
>   什么叫 用 异步的方法 去使用非阻塞调用？



### 5.9 epoll技术简介

#### 5.9.1 epoll概述

epoll技术就是高性能的代名词，epoll就是一种典型的I/O多路复用技术，epoll的最大特点是支持高并发。

传统的多路复用技术有`select()`和`poll()`没有办法实现高并发，在并发量达到1000～2000的时候，性能就会明显下降。

epoll和kqueue（macOS和freebds）类似，支持超高并发，epoll是从内核版本2.6引入的，2.6之前没有。

-   epoll和kqueue技术类似，单独一台计算机支持上万并发连接的核心技术。
-   epoll技术完全没有 「性能随着并发量提高而出现明显下降」的问题，但是没增加一个并发，必定要消耗一定的内存去保存这个连接相关的数据，多多少少有一点影响。所以不可能是无限的。
-   10W个连接同一时刻可能同一时刻只有几百个客户端给服务器发送数据；epoll只会处理这几百个客户端，因此速度很快，select和poll则会判断这10W个连接有没有数据，一次检查10W，性能收到巨大影响
-   过去很多服务器程序用多进程，每个进程对应一个连接；也有用多线程，每一个线程对应一个连接；切换「进程/线程」非常消耗资源。epoll是「事件驱动机制」，在单独的「进程/线程」里运行，「收集/处理」事件，没有「线程/进程」之间切换的消耗。
-   epoll技术非常适合高并发场景，但是写个demo和写出高质量代码融入服务器项目差别还是非常大的。



>   nginx的epoll代码质量非常高，可以学习，但是里面全是函数指针，比较复杂。



#### 5.9.1 epoll相关函数

两个Github库

-   [手写epoll: Ntytcp](https://github.com/wangbojing/NtyTcp)
    -   src下的`nty_epoll_inner.h`,`nty_epoll_rb.c`

-   [百万并发测试](https://github.com/wangbojing/c1000k_test)

`epoll_create()`,`epoll_ctl`和`epoll_wait()`,`epoll_event_callback()`是epoll的重要源码。



#### 5.9.2 `epoll_create()`函数

```cpp
#include <sys/epoll.h>
int epoll_create(int size)
```

-   功能：创建一个epoll对象，返回该对象的描述符（文件描述符），这个文件描述符对应这个epoll对象。这个对象必须用close关闭。
-   `size`参数：`Since Linux 2.6.8, the size argument is ignored, but must be greater than zero; see NOTES below.`大于0即可。

**原理：**

1.   `struct eventpoll *ep = (struct eventpoll*)calloc(1, sizeof(struct eventpoll));`分配内存，相当于new了一个eventpoll对象。

     <img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230306204537062.png" alt="image-20230306204537062" style="zoom:50%;" />

2.   eventpoll有两个成员，`rbr`和`rdlist`，`rbr`代表一颗红黑树的根节点的指针，最开始指向空。

3.   `rdlist`结构成员：是一个 「双向链表」的表头指针



**总结：**

`epoll_create()`创建一个一个`eventpoll`对象，`rbr`成员被初始化成指向一颗红黑树的根节点，`rdlist`成员被初始化成一个指向双向链表的头节点的指针。



#### 5.9.3 `epoll_ctl()`函数

```cpp
#include <sys/epoll.h>

int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

-   功能：把一个socket以及这个socket相关的事件添加到epoll对象描述符中，目的就是通过epoll对象来监视这个socket（客户端TCP连接），当有数据来往时，系统会通知我们。**对于epoll文件描述符的控制接口**。

    我们把感兴趣的事件通过`epoll_ctl()`添加到系统，当这些事件来的时候，系统会通知我们。

-   参数：

    -   `efpd`：`epoll_create()`返回的epoll对象文件描述符
    -   `op`：动作，有：添加/删除/修改， 对应宏「`EPOLL_CTL_ADD`/`EPOLL_CTL_DEL`/`EPOLL_CTL_MOD`」
        -   添加fd：等同于向红黑树上添加一个节点，红黑树的key就是 「客户端连入服务器生成的`fd`」
        -   修改事件：添加了节点之后才呢功能修改
        -   删除fd：不是删除事件，而是删除红黑树的节点，删除了对应的所有事件
    -   `fd`：表示客户端连接的`sockfd`，从`accept()`返回的fd，这个就是红黑树中的key
    -   `event`：事件信息，`op`中的`EPOLL_CTL_ADD`/`EPOLL_CTL_DEL`都需要用到这个信息

**函数原理：**

首先，红黑树的节点定义：

```c

struct epitem {
	RB_ENTRY(epitem) rbn; //rbn对应，三个指针：左孩子，右孩子，父节点，同时标注了节点的颜色
	LIST_ENTRY(epitem) rdlink; //两个指针：指向前一个节点和后一个节点，所以能连入双向链表中
	int rdy; //exist in list，这个节点是否在双向链表中

	int sockfd;
	struct epoll_event event;
};

```



>太神奇了，`epitem`既能连入红黑树，又能连入双向链表，从双向链表中移除（被复制到用户空间）后，还能在红黑树中保留着。堪称「程序设计的艺术」，稳定又高效。



-   `EPOLL_CTL_ADD`

    -   `struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp);`：查找红黑树中的节点

    -   `epi = RB_INSERT(_epoll_rb_socket, &ep->rbr, epi);`：向红黑树中插入节点

    -   `epitem.rbn`代表三个指针：左孩子，右孩子，父节点，同时标注了节点的颜色

-   `EPOLL_CTL_DEL`

    -   `struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp);`：查找红黑树中的节点
    -   `epi = RB_REMOVE(_epoll_rb_socket, &ep->rbr, epi);` ,删除红黑树的节点

-   `EPOLL_CTL_MOD`

    -   `struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp);`：查找红黑树中的节点

    -   ```cpp
        epi->event.events = event->events;
        epi->event.events |= EPOLLERR | EPOLLHUP;
        ```

        修改节点的`events`成员。

所以如果有100W个并发，也就要添加100W个文件描述符到epoll实例，具体来说是红黑树中。



**总结：**

-   `EPOLL_CTL_ADD`等价于往红黑树中增加节点
-   `EPOLL_CTL_DEL`等价于从红黑树中删除节点
-   `EPOLL_CTL_MOD`等价于修改已有的红黑树节点，修改的依据就是`sockid`（`accept()`产生的文件描述符）

当事件发生，我们如何获取OS给我们的通知呢？



#### 5.9.4 `epoll_wait()`函数

epoll只会遍历双向链表中的数据。

```cpp
//epoll_wait, epoll_pwait - wait for an I/O event on an epoll file descriptor
#include <sys/epoll.h>
int epoll_wait(int epfd, struct epoll_event *events,
               int maxevents, int timeout);
```

-   功能：阻塞一小段时间，并等待事件发生，返回事件集合，也就是获取内核的事件通知。说白了就是遍历「双向链表」`rdlist`，把这个双向链表里面的节点拷贝出去，拷贝完毕的就从双向链表中移除。

    双向链表中记录的是所有有数据（有事件）的`socket`（TCP连接）。

-   参数：

    -   `epfd`：从`epoll_create()`中返回的epoll对象描述符
    -   `events`：是内存，也是数组，长度是`maxevents`，是一个传出参数，从双向链表拷贝出来的数据就存在这里，如果双向链表中的数据大于`maxevents`，则只会拷贝最大大小的数据，所以需要传入`maxevents`参数。
    -   `maxevents`：上面已经解释了
    -   `timeout`：阻塞等待的时长，单位是ms，如果双向链表中有数据，可能会立即返回。



**原理：**

-   `int num = (ep->rdnum > maxevents ? maxevents : ep->rdnum);`：确定传入的`events`是否足够容下所有需要复制的事件大小（`ep->denum`），如果不够就用`maxevents`
-   `while (num != 0 && !LIST_EMPTY(&ep->rdlist))`循环等待timeout
    -   每一次循环都取双向链表的头节点`struct epitem *epi = LIST_FIRST(&ep->rdlist);`
    -   从双向链表中移除第一个节点`LIST_REMOVE(epi, rdlink);`



#### 5.9.5 内核向双向链表中插入节点

一般有四种情况，会使操作系统把节点插入双向链表中：

-   客户端完成三次握手，服务器要`accept()`
-   客户端关闭连接，服务器要调用`close()`回收资源
-   客户端发送来数据，服务器要调用`read()/recv()`
-   当可以发送数据时，服务器可以调用`send()/write()`。如果服务器发送数据非常快，客户端收数据很慢，此时需要通知服务器
-   其他情况

OS在必要的时候调用`int epoll_event_callback(struct eventpoll *ep, int sockid, uint32_t event)`往双向链表写入数据，过程如下：

-   `struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp);` ，从红黑树中查找节点
-   判断这个节点是否在双向链表中，`epi->event.events |= event;`，把新发生的事件加到事件中
-   把来事件的节点放到双向链表表头的位置

### 5.10 epoll的LT和ET

-   LT：level trigger，水平触发，这种工作模式 是低速模式（效率差），缺省是LT模式
-   ET：edge trigger，边缘触发，这种工作模式，是高速模式（效率高）

`ev.events | EPOLLET`就是开启ET。

水平触发：来一个事件， 如果你不处理它，那么这个事件就会一直被触发。所谓处理，就是用一个接收函数来处理（`accept4()`函数）。

边缘触发：只对非阻塞socket有效，来一个事件，内核只会通知一次。不管是否处理，内核都不再处理。提高了系统运行效率，缺点是编码的难度加大。因为只通知一次，所以接到通知后需要保证把该处理的事情处理好（一次要把所有的数据读取完毕）。



### 5.11 服务器设计理论

#### 5.11.1 服务器设计原则总述

本项目是一个通用服务器框架，稍加改造就可以应用在具体的开发工作中。

#### 5.11.2 TCP粘包、缺包

TCP粘包问题：

client发送abc，def，hij三个数据包发送出去：

-   **客户端粘包现象：**

    客户端因为有一个Nagle优化算法，可能会把多次`send()`/`write()`数据包合并成一个数据包发送出去。这样效率会更高，客户端是可以使用某个函数关闭Nagle优化算法，这样调用几次`send()`就会发送几个包。

-   **服务器端粘包、缺包现象：**

    不管客户端是否粘包，服务器端都存在粘包问题；所以客户端没有太大必要解决粘包问题。

    -   因为服务器的本次`recv()`和下次`recv()`存在时间间隔，可能在这个时间间隔之内，收到了多个客户端发送的包。服务器对于每个连接都有收发缓冲区，这多个包都被存在「收数据缓冲区」，所以出现粘包现象。
    -   再举一例：客户端发送一个超长的包，操作系统会将其拆成多个包发送，网络如果此时出现延迟/阻塞。服务器端`recv()`可能会先收到一部分，再次`recv()`收到一部分，。。。如此反复，直到多次（可能多达几十次）`recv()`收完，如果包不完整，可以称之为「缺包」

在局域网测试，一般不会出现上述问题，但是在互联网上，则是一个必须解决的问题。

#### 5.11.3 TCP粘包、缺包解决



实际上，所谓TCP粘包，就是「在TCP传输协议下，应用层数据拼装发送和接收解析问题。」设计一个带包头的应用层报文结构，客户端和服务端都遵循这个结构，就能解决所谓粘包。TCP本质就只负责传输数据，是一个流式协议，把stream转为datagram本身就是程序员需要做的。

**粘包：**要解决的就是把这些包拆出来，需要「简单、严谨、有效」，比如在包结尾加一个特殊字符`$`用来分割，这种方法就非常不严谨，容易被恶意数据包攻击（一个原则就是不能假设数据包都是合理的）。

可以这样解决粘包问题：给收发的数据包定义一个统一的格式：C/S都按照这个格式来，就能解决粘包问题：

-   包格式：包头+包体的格式：其中包头是固定长度，在包头中，有一个字段记录整个包（包头+包体）的长度
-   这样的话，包头是定长的，我们可以从包头得到整个包的长度，我们可以用「整个包的长度」-「包头的固定长度」得到包体的长度，我们再接收「包体长度」的字节数，服务器就认为一个完整的数据包收完。就可以处理数据了。



>   nginx官方代码用来作为web服务器，web服务器是没有固定格式服务器，需要一直收发包。



## 6. 网络通讯实战

### 6.1 监听端口

在`net/ngx_c_sockt`中定义`CSocekt`类：

```cpp
//socket相关类
class CSocekt
{
public:
	CSocekt();                                                         //构造函数
	virtual ~CSocekt();                                                //释放函数
public:    
    virtual bool Initialize();                                         //初始化函数

public:	
	int  ngx_epoll_init();                                             //epoll功能初始化
	//void ngx_epoll_listenportstart();                                  //监听端口开始工作 
	int  ngx_epoll_add_event(int fd,int readevent,int writeevent,uint32_t otherflag,uint32_t eventtype,lpngx_connection_t c);     
	                                                                   //epoll增加事件
	int  ngx_epoll_process_events(int timer);                          //epoll等待接收和处理事件

private:	
	void ReadConf();                                                   //专门用于读各种配置项	
	bool ngx_open_listening_sockets();                                 //监听必须的端口【支持多个端口】
	void ngx_close_listening_sockets();                                //关闭监听套接字
	bool setnonblocking(int sockfd);                                   //设置非阻塞套接字	

	//一些业务处理函数handler
	void ngx_event_accept(lpngx_connection_t oldc);                    //建立新连接
	void ngx_wait_request_handler(lpngx_connection_t c);               //设置数据来时的读处理函数

	void ngx_close_accepted_connection(lpngx_connection_t c);          //用户连入，我们accept4()时，得到的socket在处理中产生失败，则资源用这个函数释放【因为这里涉及到好几个要释放的资源，所以写成函数】

	//获取对端信息相关                                              
	size_t ngx_sock_ntop(struct sockaddr *sa,int port,u_char *text,size_t len);  //根据参数1给定的信息，获取地址端口字符串，返回这个字符串的长度

	//连接池 或 连接 相关
	lpngx_connection_t ngx_get_connection(int isock);                  //从连接池中获取一个空闲连接
	void ngx_free_connection(lpngx_connection_t c);                    //归还参数c所代表的连接到到连接池中	

private:
	int                            m_worker_connections;               //epoll连接的最大项数
	int                            m_ListenPortCount;                  //所监听的端口数量
	int                            m_epollhandle;                      //epoll_create返回的句柄

	//和连接池有关的
	lpngx_connection_t             m_pconnections;                     //注意这里可是个指针，其实这是个连接池的首地址
	lpngx_connection_t             m_pfree_connections;                //空闲连接链表头，连接池中总是有某些连接被占用，为了快速在池中找到一个空闲的连接，我把空闲的连接专门用该成员记录;
	                                                                        //【串成一串，其实这里指向的都是m_pconnections连接池里的没有被使用的成员】
	//lpngx_event_t                  m_pread_events;                     //指针，读事件数组
	//lpngx_event_t                  m_pwrite_events;                    //指针，写事件数组
	int                            m_connection_n;                     //当前进程中所有连接对象的总数【连接池大小】
	int                            m_free_connection_n;                //连接池中可用连接总数


	std::vector<lpngx_listening_t> m_ListenSocketList;                 //监听套接字队列

	struct epoll_event             m_events[NGX_MAX_EVENTS];           //用于在epoll_wait()中承载返回的所发生的事件

};
```



#### 6.1.1 开启监听端口

`Initialize()`调用`ngx_open_listening_sockets()`, 后者用来开启端口，在创建worker进程之前就要执行这个函数。（为什么？，这样work进程就都会监听80和443端口）



主要围绕`CSocket`类来编写代码。

### 6.2 `ngx_epoll_init()`函数

#### 6.2.1 连接池

所谓连接池，就是一个数组，元素数量为`work_connections`，每个数组元素类型为`ngx_connection_t`。为什么要引入这个数组，例如80和443有两个监听套接字，用户连入进来，每个用户都会增加一个套接字。

```cpp

typedef struct ngx_connection_s  ngx_connection_t,*lpngx_connection_t;
typedef struct ngx_listening_s  //和监听端口有关的结构
{
	int            port;   //监听的端口号
	int            fd;     //套接字句柄socket
    lpngx_connection_t        connection;  //连接池中的一个连接，注意这是个指针 
}ngx_listening_t,*lpngx_listening_t;
//以下三个结构是非常重要的三个结构，我们遵从官方nginx的写法；
//(1)该结构表示一个TCP连接【客户端主动发起的、Nginx服务器被动接受的TCP连接】
struct ngx_connection_s
{
	
	int                       fd;             //套接字句柄socket
	lpngx_listening_t         listening;      //如果这个链接被分配给了一个监听套接字，那么这个里边就指向监听套接字对应的那个lpngx_listening_t的内存首地址		

	//------------------------------------	
	unsigned                  instance:1;     //【位域】失效标志位：0：有效，1：失效【这个是官方nginx提供，到底有什么用，ngx_epoll_process_events()中详解】  
	uint64_t                  iCurrsequence;  //我引入的一个序号，每次分配出去时+1，此法也有可能在一定程度上检测错包废包，具体怎么用，用到了再说
	struct sockaddr           s_sockaddr;     //保存对方地址信息用的
	//char                      addr_text[100]; //地址的文本信息，100足够，一般其实如果是ipv4地址，255.255.255.255，其实只需要20字节就够

	//和读有关的标志-----------------------
	//uint8_t                   r_ready;        //读准备好标记【暂时没闹明白官方要怎么用，所以先注释掉】
	uint8_t                   w_ready;        //写准备好标记

	ngx_event_handler_pt      rhandler;       //读事件的相关处理方法
	ngx_event_handler_pt      whandler;       //写事件的相关处理方法
	
	//--------------------------------------------------
	lpngx_connection_t        data;           //这是个指针【等价于传统链表里的next成员：后继指针】，指向下一个本类型对象，用于把空闲的连接池对象串起来构成一个单向链表，方便取用
};

```



需要把套接字数字和一块内存捆绑，达到的效果就是：「通过这个套接字，就能把这块内存拿出来」。 

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230307115032647.png" alt="image-20230307115032647" style="zoom:50%;" />

初始化的时候，需要把连接池作为「既是一个链表，又是一个数组」的结构。nginx把空闲的元素连接成链表（初始化的时候全是空闲的），然后在需要的时候，只需要从链表中取出第一个元素，然后「头指针」指向下一个元素即可。始终保持链表中全是空闲的元素，这样在连接池中找一个空闲的连接就非常快。回收的时候，把回收的元素放到链表的最前面，同时改变「`m_pfree_connections`」的指向。

#### 6.2.2 `ngx_epoll_init()`内容

**`ngx_epoll_init()`的内容：**

1.   `epoll_create()`：创建一个epoll对象（一个红黑树，一个双向链表）
2.   创建连接池（数组），同时遍历监听端口（80、443），为每个监听socket增加一个 连接池中的连接（就是让一个socket和一个内存绑定，以方便记录该sokcet相关的数据、状态等等），调用`ngx_epoll_add_event()`添加监听的事件（只关心读事件）。

这样就做好了准备，等待客户端主动发起三次握手连入。



>   **位域：**
>
>   `unsigned instance:1`，`instance`只占一位。可以把一个指针一个位域合二为一，放入`void*`中。



**在`net/ngx_c_socket_conn.cxx`中有一些重要函数：**

-   `lpngx_connection_t CSocekt::ngx_get_connection(int isock)`：从连接池中获取一个空闲连接，并把它的`fd`设置为传入`isock`
-   `ngx_free_connection(lpngx_connection_t c) `：回收一个连接，直接把这个放到链表的头



>   可以用`lsof -i:80`查看哪些进程在监听80端口，`netstat`只能知道80端口被监听，但是不知道是哪个进程。



#### 6.2.3 `ngx_epoll_init()`调用时机

`nginx_epoll_init()`函数在哪里调用？

要在子进程中功能执行，和父进程没有关系，调用顺序如下：


-    ngx_master_process_cycle()        //创建子进程等一系列动作

     -	ngx_setproctitle()            //设置进程标题    
     
     -	ngx_start_worker_processes()  //创建worker子进程   
     
         -	for (i = 0; i < threadnums; i++)   //master进程在走这个循环，来创建若干个子进程
     
             -	ngx_spawn_process(i,"worker process");
     
                 -	pid = fork(); //分叉，从原来的一个master进程（一个叉），分成两个叉（原有的master进程，以及一个新fork()出来的worker进程
     
                 -	//只有子进程这个分叉才会执行ngx_worker_process_cycle()
     
                     ngx_worker_process_cycle(inum,pprocname);  //子进程分叉
     
                     -	ngx_worker_process_init();
                         -	sigemptyset(&set);  
                         -	sigprocmask(SIG_SETMASK, &set, NULL); //允许接收所有信号
                         -	g_socket.ngx_epoll_init();  //初始化epoll相关内容，同时 往监听socket上增加监听事件，从而开始让监听端口履行其职责
                             -	m_epollhandle = epoll_create(m_worker_connections);
                             -	ngx_epoll_add_event((*pos)->fd....);
                                 -	epoll_ctl(m_epollhandle,eventtype,fd,&ev);
                     -	ngx_setproctitle(pprocname);          //重新为子进程设置标题为worker-process，到此子进程就准备好了，可以开始工作了
                     -	for ( ;; ) {}. ....                   //子进程开始在这里不断的死循环
     
     -	sigemptyset(&set); 
     
     -	for ( ;; ) {}.                //父进程[master进程]会一直在这里循环 

>   **一个问题：**
>
>   为什么只有子进程调用了`epoll_ctl()`，而`lsof -i:80`却显示子进程和父进程都监听了80端口？
>
>   因为监听端口在父进程中进行。



#### 6.2.4 `ngx_epoll_add_event()`函数

在该函数中，定义了一个结构体`struct epoll_event ev`变量。`ev`中有两个成员：

-   `events`表示感兴趣的事件类型，包含包括可读（`EPOLLIN`）、可写（`EPOLLOUT`）、对端关闭（`EPOLLRDHUP`）、错误（`EPOLLERR`）等事件。我们根据需要使用即可
-   `data`则是一个union，可以是`fd`，也可以是其他相关数据，我们在这里，将`( (uintptr_t)c | c->instance);`给它。这里是用了一个编程小技巧。

c是 一个 `ngx_connection_s`结构体的指针，这个结构体表示一个TCP连接，它里面包含了`fd`（套接字句柄）、`instance`位域（代表是否有效）、`s_sockaddr`对方的地址信息、`rhandler`读事件的处理方法、`whandler`写事件的相关处理方法等信息，同时这个结构体还是一个链表的节点，它的`data`指针指向下一个同类型结构体（相当于`next`)。

### 6.3 `nxg_epoll_process_events()`

如何获取到用户的连入事件？同时把用户接入进来？

使用`net/ngx_c_socket.cxx/ngx_epoll_process_events()`函数。

#### 6.3.1 `ngx_epoll_process_events()`调用位置

在`ngx_worker_process_cycle()`的for循环中调用，用来处理网络事件和定时器事件。 

所以：

-   这个函数，仍然是在子进程中被调用
-   这个函数在死循环中，所以它会被不断地调用



#### 6.3.2 事件驱动

官方nginx本身的架构也被称为「事件驱动架构」。什么是「事件驱动」？

-   「驱动」：动力的来源，「驱动」的问题就是探讨程序如何工作的问题
-   「事件驱动」：就是通过获取事件，通过获取到的事件来调用适当的函数从而让整个程序工作。

**总结：**

所谓事件驱动框架，就是由一些事件发生源（三次握手内核通知，事件源就是客户端），通过事件收集器来收集和分发事件：

-   `epoll_wait()`函数就是nginx的事件收集器
-   分发事件就是把事件交给事件处理器(`ngx_wait_request_handler()`和`ngx_epoll_process_event()`)

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230307202338299.png" alt="image-20230307202338299" style="zoom:50%;" />

上图不能有阻塞行为。

#### 6.3.3 `ngx_epoll_process_events()`函数内容

用户三次握手成功，连入进来，所谓「连入进来」的事件，对于服务器而言，就是 一个监听套接字上的可读事件。

`epoll_wait()`就是从双向链表中获取事件（还有对应的套接字）。

`nxg_epoll_process_event()`首先调用`epoll_wait()`等待事件，并返回`events`（事件的个数，是`epoll_events`数组/指针），然后遍历`events`，取出`m_events[i].data.ptr`（被放入红黑树的连接池对象，里面有很多信息），并判断`m_events[i].events`（事件类型），如果是读事件，则调用对应的读处理函数`rhandler`（存储在连接池对象中）

### 6.4 `ngx_event_accept()`函数内容

这个函数是「监听套接字」 读处理事件的处理函数（这是在`ngx_epoll_init()`中设置好的，连接池对象中，监听套接字和连接套接字的`rhandler`不同，这是一个函数指针，代表读事件（`EPOLLIN`）来临时的处理函数）。

`ngx_event_accept()`所要做的就是，将监听套接字的读事件（其实就是有新TCP连接到来）处理，正常情况下的处理步骤如下：

-   调用`accept4()/accept()`，返回连接的套接字，得到客户端的地址信息，`accept4()`非阻塞。
-   调用`ngx_get_connection()`，加入连接池中
-   初始化连接池的一些成员，例如监听对象、设置 读数据的处理函数`nag_wait_request_handler()`、拷贝客户端地址到`s_sockaddr`成员

对于监听套接字，用的LT模式，防止出现用户没有接入进来；

所有的接入进来的用户套接字都是 ET模式（暂时是ET，后面改为LT模式）。

`ngx_wait_request_handler()`是连接套接字的处理函数





#### 6.4.1 测试

来数据会调用`ngx_wait_request_handler()`，可以在里面打印一个日志。



### 6.5 ET、LT模式测试和分析

#### 6.5.1 ET、LT测试



LT模式没处理完就会被一直触发，ET则只会出现一次。

普遍认为，ET比LT效率要高一些，但是ET编写代码的难度要大一些：

-   客户端一次发送许多数据在服务器的收缓冲区存着, 但是ET模式可能只会收取一部分
-   如果客户端继续发送数据，又触发一次读事件，服务器继续收取上次没有收取完的数据

可能会想，那么我每次用一个循环收取数据呢？那么有可能没有数据接收， `recv()`返回-1，可以使用这个条件作为循环跳出的条件，ET模式的接收数据代码：

```cpp
do
{
    int n = recv(c->fd, buf, 2, 0);
    if(n == -1 && errno == EAGAIN)
        break;//数据收完了
    if(n == 0) //客户端断开连接
        break;
    ngx_log_stderr(0, "OK, 收到的字节数为%d, 内容为%s\n", n, buf);
}while(1);
```

而LT模式接收数据：

```cpp
unsigned char buf[10] = {0};
memset(buf, 0, sizeof(buf));
int n = recv(c->fd, buf, 2, 0);
if(n == 0)
{
    //客户端断开连接
    //服务器端的处理
    ngx_free_connection(c);
    close(c->fd);
    c->fd = -1;
}
ngx_log_stderr(0, "OK, 收到的字节数为%d, 内容为%s\n", n, buf);
```

LT模式对于客户端断开处理时，需要关闭`fd`，否则会一直未处理客户端断开的事件。



>   **思考：**
>
>   为什么ET模式事件只触发一次，LT模式事件会触发多次呢？
>
>   epoll针对ET和LT的处理方式不同，在LT模式下，只要有数据，双向链表中就一定有连接；在ET模式下，一次`epoll_wait()`取走数据，双向链表就没有这个数据了。
>
>   如果在水平模式下，快速地把数据读取完，LT的效率不会比ET差太多。（只是一个猜想）





#### 6.5.2 如何选择ET、LT

如果收发数据包有「固定格式」，编程简单、清晰，那么LT模式效率并不一定低。

本项目采用LT，如果收发数据包没有固定格式，可以考虑使用ET模式。



### 6.6 收发包实战

#### 6.6.1 分析及包头结构定义

发包：采用 包头+包体，其中包头中记录着整个包（包头+包体）的长度。`_include/ngx_comm.h`定义了包的结构：

```cpp
//结构定义------------------------------------
#pragma pack (1) //对齐方式,1字节对齐【结构之间成员不做任何字节对齐：紧密的排列在一起】

//一些和网络通讯相关的结构放在这里
//包头结构
typedef struct _COMM_PKG_HEADER
{
	unsigned short pkgLen;    //报文总长度【包头+包体】--2字节，2字节可以表示的最大数字为6万多，我们定义_PKG_MAX_LENGTH 30000，所以用pkgLen足够保存下
	                            //包头中记录着整个包【包头—+包体】的长度

	unsigned short msgCode;   //消息类型代码--2字节，用于区别每个不同的命令【不同的消息】
	int            crc32;     //CRC32效验--4字节，为了防止收发数据中出现收到内容和发送内容不一致的情况，引入这个字段做一个基本的校验用	
}COMM_PKG_HEADER,*LPCOMM_PKG_HEADER;


#pragma pack() //取消指定对齐，恢复缺省对齐

```



包头有一些条件：

-   一个包的长度不能超过30000个字节，必须要有最大值（防止恶意数据包，它规定这个包有300亿字节）。这个规定能够确保服务器不会处于非常危险的境地
-   开始定义包头结构：`_COMM_PKG_HEADER`
-   **注意结构字节对齐问题**，不同操作系统的结构对齐方式可能不同，导致C/S通信收发不一致。所以，在网络上发送结构体，都采用1字节对齐。`#pargma pack (1)`。这样结构中的成员都是紧密结合在一起



#### 6.6.2 收包状态宏定义

收包思路：先收包头，根据包头中的内容确定包体长度并收包，需要几个收包状态（状态机）：

```cpp
//通信 收包状态定义
#define _PKG_HD_INIT         0  //初始状态，准备接收数据包头
#define _PKG_HD_RECVING      1  //接收包头中，包头不完整，继续接收中
#define _PKG_BD_INIT         2  //包头刚好收完，准备接收包体
#define _PKG_BD_RECVING      3  //接收包体中，包体不完整，继续接收中，处理后直接回到_PKG_HD_INIT状态

```

每一个TCP连接都是上面状态的一种，所以连接池结构需要添加一个的字段，用来记录收包的状态，所以初始化的时候也要初始化收包的状态为`0`初始状态。

socket一定要和一块内存绑定。



#### 6.6.3 收包实战代码

定义一个固定大小的数组来接收数组。在连接池成员额外增加如下成员：

```cpp

//和收包有关
unsigned char             curStat;                        //当前收包的状态
char                      dataHeadInfo[_DATA_BUFSIZE_];   //用于保存收到的数据的包头信息			
char                      *precvbuf;                      //接收数据的缓冲区的头指针，对收到不全的包非常有用，看具体应用的代码
unsigned int              irecvlen;                       //要收到多少数据，由这个变量指定，和precvbuf配套使用，看具体应用的代码

bool                      ifnewrecvMem;                   //如果我们成功的收到了包头，那么我们就要分配内存开始保存 包头+消息头+包体内容，这个标记用来标记是否我们new过内存，因为new过是需要释放的
char                      *pnewMemPointer;                //new出来的用于收包的内存首地址，和ifnewrecvMem配对使用

```

这个连接刚建立的时候`ngx_get_connection()`，处于收包头的状态，实战代码聚焦在`ngx_wait_request_handler()`函数，同时设置好收包的状态。

我们要求客户端连入到服务器后，要主动（客户端的义务）给服务器发送数据包，服务器要主动收客户端的数据包。

引入一个消息头的概念，这也是一个结构体，用来记录一些额外信息，我们在服务器收到包的时候额外附加一个消息头：消息头+包头+包体，用来处理一些过时包的问题。

```cpp
//消息头，引入的目的是当收到数据包时，额外记录一些内容以备将来使用
typedef struct _STRUC_MSG_HEADER
{
    lpngx_connection_t pConn;         //记录对应的链接，注意这是个指针
    uint64_t           iCurrsequence; //收到数据包时记录对应连接的序号，将来能用于比较是否连接已经作废用
    //......其他以后扩展	
}STRUC_MSG_HEADER,*LPSTRUC_MSG_HEADER;
```



还需要一个分配和释放内存的类，因为在收包体的时候，需要new包体的大小，在`misc/ngx_c_memory.cxx`中有类`CMemory`，有两个成员函数`AllocMemory()`和`FreeMemory()`，用来分配和释放内存。

没有使用内存池，对于提高程序运行效率有限，单纯的`new`也很快了，内存池的主要功能就是频繁分配小块内存时， 内存池可以节省额外内存开销。



#### 6.6.4 测试

测试服务器`ngx_wait_request_handler()`函数是否正常工作。



## 7. 服务器业务逻辑处理框架

### 7.1 多线程的提出

用「线程」来解决客户端发过来的数据包，一个进程 跑起来之后，就自动启动了一个「主线程」，也就是说一个worker进程一启动，就等于只有一个「主线程」在跑。

涉及到了业务逻辑层面，就要用多线程处理。例如游戏充值业务，需要本服务器与专门的充值服务器通讯，此时必须采取多线程处理方式。一个线程因为充值被卡住，还有其他线程可以提供给其他玩家及时服务。

所以，服务器端处理用户需求（用户逻辑）的时候，一般都会启动几十甚至上百个线程来处理，以保证用户的需要能够得到及时处理。

epoll根本就没有使用多线程（至少本项目没有）。

主线程收到一个完整的包后，会把它放入消息队列中（调用``inMsgRecvQueueAndSignal()`），有一堆线程要从这个消息队列中取走这个包，所以必须要用互斥，在同一时刻，只有一个线程操作消息队列。

**一些概念：**

-   POSIX：表示可移植操作系统接口（Portabl Operating System Interface of Unix）
-   POSIX线程：是POSIX的线程标准，这个标准定义了创建和操作线程的一套API（Application Programming Interface），也就是定义了一系列函数，一般以`pthread_`开头，比较成熟。



### 7.2 线程池实战之创建线程池

#### 7.2.1 引入线程池

为什么引入线程池？

完全不推荐使用单线程的方式解决逻辑业务问题，推荐多线程开发。

**线程池：**提前创建好一堆线程，并且用一个类来统一管理和调度这些线程，这些线程就像一个池子一样，称为「线程池」。

-   当来了一个任务（消息）的时候，我从这一堆线程中找一个空闲的线程去做这个任务（处理这个消息）。
-   任务完成后，线程中有一条循环语句，在这个循环语句中等待新任务，有新任务的时候再去执行新的任务。
-   就好像这个线程可以回收再利用一样。

>   **线程池的意义和价值：**
>
>   -   事先创建好一堆线程，避免了动态创建线程来执行任务，提高了程序的稳定性（在程序刚运行的时候，可用资源较多）。有效规避程序运行之中创建线程有可能失败的风险。
>   -   提高程序运行效率：线程池中的线程，反复循环再利用。
>   -   管理容易，代码更加清晰简单。



#### 7.2.2 线程池的初始化和销毁

在 `misc/ngx_c_threadpool.cxx`文件中。

使用`pthread`需要在编译的时候加`-lpthread`或`-pthread`。

调用`pthread_create()`函数创建线程，并开始执行「线程入口函数」。

线程池管理类`CThreadPool`，其中有`Create()`和`ThreadFunc()`两个重要函数。

`Create()`函数在`ngx_worker_process_init()`中被调用，每个worker进程中都要创建一个线程池。

`pthread_cond_wait()`和`pthread_boardcast()`函数

`ptread_join()`函数

`StopALL()`能够停止所有线程



#### 7.2.3 线程池的激发

所谓「激发」，就是让线程池开始干活，当我收到一个完整的用户来的消息的时候，就要激发「线程池来获取消息」开始工作。

激发的代码应该放在消息放到消息队列之后，也就是`inMsgRecvQueueAndSignal()`之后，激发函数是`Call()`，创建一个线程会立刻进入线程的入口函数`ThreadFunc()`

`pthread_cond_signal()`能激发 卡在`pthread_cond_wait()`的线程。



#### 7.2.4 线程池的完善和测试

测试的代码写在`test/testClient.cpp`中。

只开一个进程和一个线程，同时发送多个消息，会堆积，但是不会丢失消息，消息会逐条处理，我们可以写入日志，说明线程池太少了。

如果调用一个`pthread_cond_signal()`可能唤醒多个线程，这个叫「惊群」，也叫虚假唤醒，但是只有一个线程拿到互斥量。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230309205038040.png" alt="image-20230309205038040" style="zoom:50%;" />

如果开两个线程，就可以同时处理两个任务。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230309204652252.png" alt="image-20230309204652252" style="zoom:50%;" />



`m_pthreadMutex`是一个互斥量，用来完成收发消息。

使用线程池`g_threadpool.inMsgRecvQueueAndSignal()`函数可以把消息送入消息队列，调用`Call()`的功能来唤醒一个线程。

所以收消息队列相关的变量也放到`CThreadPool`类中来。

这个函数一进入就加锁，操作完消息队列的时候，就解锁。

支撑线程池的运行主要靠两个函数:`pthread_cond_signal()`和`pthread_cond_wait()`。



>   **《APUE》11.6.6：**
>
>   条件变量是线程可用的另一种同步机制，条件变量给多线程提供了一个会合的场所，条件变量和互斥量一起使用的时候，允许线程以无竞争的方式等待特定的条件发生。
>
>   -   条件本身（可以理解成while循环，而不是条件变量）是由互斥量保护的，线程在改变条件状态之前必须锁住互斥量，其他线程在获取到互斥量之前不会觉察到这种改变，因为互斥量必须在锁定以后才能计算条件。
>   -   C++11中的条件变量没有这么抽象，而是需要定义一个类。



传递给`pthread_connnd_wait()`函数的互斥量`m_pthreadMutex`对条件` while ( (pThreadPoolObj->m_MsgRecvQueue.size() == 0) && m_shutdown == false)`进行保护。

调用者把锁住的互斥量传递给函数`pthread_cond_wait()`，函数然后自动把调用线程放在等待条件的线程列表上，对互斥量解锁，这就关闭了「条件检查」和「线程进入休眠状态等待条件改变」这两个操作之间的时间通道，这样线程就不会错过条件的任何变化。`pthread_cond_wait()`返回时，互斥量再次被锁定。

`pthread_cond_wait()`被唤醒就会去尝试去拿锁，没拿到就会不断去拿，如果是多个CPU，可能有惊群。



### 7.3 线程池代码之业务逻辑

#### 7.3.1 一个简单的crc32算法介绍

引入两个文件`misc/ngx_c_crc32.cxx`和`_include/ngx_c_crc32.h`，用来实现crc32算法（`CCRC32`类）。目的是对收发数据包进行一个简单的校验，确保数据包中的内容没有被篡改过。

-   只使用`GetCRC()`接口，传入一个字符串和字符串长度，这个函数计算出一个`int`类型的数字并返回。

客户端发送数据包给服务器的时候，客户端会把包体的内容计算出一个`CRC32`的值，放在包头里。服务器收到包体之后，也会计算一个包体的`CRC32`的值，并与客户端发送过来的`CRC32`的值进行对比。服务器发送给客户端的数据，客户端也应该相同处理。



#### 7.3.2 引入新的`CSocket`子类

真正的项目中，要把`CSocket`类当作父类使用，具体业务逻辑，应该放在`CSocket`的子类中。子类放在`logic/`目录下，本项目的一个子类就是`CLogixSocket`类，它`public`继承了`CSocket`类。



线程池调用的`threadRecvProcFunc()`收到消息之后的处理函数也在`CSoket`中。



#### 7.3.3 消息的具体设计

```cpp
//包头结构
typedef struct _COMM_PKG_HEADER
{
	unsigned short pkgLen;    //报文总长度【包头+包体】--2字节，2字节可以表示的最大数字为6万多，我们定义_PKG_MAX_LENGTH 30000，所以用pkgLen足够保存下
	                            //包头中记录着整个包【包头—+包体】的长度

	unsigned short msgCode;   //消息类型代码--2字节，用于区别每个不同的命令【不同的消息】
	int            crc32;     //CRC32效验--4字节，为了防止收发数据中出现收到内容和发送内容不一致的情况，引入这个字段做一个基本的校验用	
}COMM_PKG_HEADER,*LPCOMM_PKG_HEADER;
```

其中`msgCode`用来区别不同的命令。

在`ngx_c_slogic.cxx`中定义一个成员函数指针：

```cpp
//定义成员函数指针
typedef bool (CLogicSocket::*handler)(  lpngx_connection_t pConn,      //连接池中连接的指针
                                        LPSTRUC_MSG_HEADER pMsgHeader,  //消息头指针
                                        char *pPkgBody,                 //包体指针
                                        unsigned short iBodyLength);    //包体长度
```

为了能够根据客户端发送过来的消息代码，迅速定位到要执行的函数，我们就把客户端发送过来的消息代码，直接当作一个下标来用。

我们定义一个成员函数数组，数组的下表就是包头中的`msgCode`。

```cpp

//用来保存 成员函数指针 的这么个数组
static const handler statusHandler[] = 
{
    //数组前5个元素，保留，以备将来增加一些基本服务器功能
    NULL,                                                   //【0】：下标从0开始
    NULL,                                                   //【1】：下标从0开始
    NULL,                                                   //【2】：下标从0开始
    NULL,                                                   //【3】：下标从0开始
    NULL,                                                   //【4】：下标从0开始

    //开始处理具体的业务逻辑
    &CLogicSocket::_HandleRegister,                         //【5】：实现具体的注册功能
    &CLogicSocket::_HandleLogIn,                            //【6】：实现具体的登录功能
    //......其他待扩展，比如实现攻击功能，实现加血功能等等；


};
#define AUTH_TOTAL_COMMANDS sizeof(statusHandler)/sizeof(handler) //整个数组有多少个项，编译时即可知道
```

所以服务端和客户端要约定好，通过客户端发送过来的`msgCode`，可以迅速定位到 成员函数。以后每增加一个功能，就在数组中添加一个项。每个 业务逻辑处理函数都需要三个参数：

-   连接头中的`lpngx_connection_t`指针，指向连接池中的一项
-   消息头
-   包体
-   包体的长度



同时在`ngx_logiccomm.h`头文件中，也要加入客户端类似的通讯用的结构：

```cpp
#ifndef __NGX_LOGICCOMM_H__
#define __NGX_LOGICCOMM_H__

//结构定义------------------------------------
#pragma pack (1) //对齐方式,1字节对齐【结构之间成员不做任何字节对齐：紧密的排列在一起】

typedef struct _STRUCT_REGISTER
{
	int           iType;          //类型
	char          username[56];   //用户名 
	char          password[40];   //密码

}STRUCT_REGISTER, *LPSTRUCT_REGISTER;

typedef struct _STRUCT_LOGIN
{
	char          username[56];   //用户名 
	char          password[40];   //密码

}STRUCT_LOGIN, *LPSTRUCT_LOGIN;


#pragma pack() //取消指定对齐，恢复缺省对齐

#endif
```

这样服务器就可以解析客户端发送过来的结构。

>   **最终认识：**
>
>   服务器的业务逻辑开发，主要集中在三个文件中：
>
>   -   `ngx_logiccomm.h`增加通信的头信息结构体
>   -   `ngx_c_slogic.cxx`和`ngx_c_slogic.h`增加成员函数来处理额外的业务逻辑



#### 7.3.4 `threadRecvProcFunc()`函数

`CLogicSocket::threadRecvProcFunc(char *pMsgBuf)`函数用来处理收到的数据包，参数代表「消息头+包头+包体」。

它用来判断包头的参数是否符合规范，进行CRC检验，没有问题就送入对应的 成员函数执行具体的业务逻辑。



#### 7.3.5 测试

服务器应该制定和客户端通信的协议。

需要修改客户端的代码，需要注意，客户端不要立即断开，也就是`close(fd)`，因为服务器会判断客户端是否断开，如果断开，就丢弃这个包。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230310172444738.png" alt="image-20230310172444738" style="zoom:50%;" />

至此，框架已经搭建完毕，可以添加业务逻辑详细代码。





#### 7.3.6 业务逻辑详细代码

`_HandleRegister()`和`_HandleLogIn()`里面干嘛，可以自己发挥。





### 7.4 连接池中连接回收思考

#### 7.4.1 连接池回收可能存在问题

服务器：理论上7 * 24小时不间断。必须稳定。

连接池连接回收存在隐患。

`ngx_close_connection()`是被`recv`调用，用来回收连接池中的连接。如果客户端掉线，服务端立即回收连接，这个连接很有可能被紧随其后的新客户端所使用。

-   客户端A给服务器发送一个信息，需要执行某个函数，假如这个函数执行时间比较久（10秒），服务器要从线程池中找一个线程来执行A的任务，如果执行到第5秒，A断线了，服务器通过`epoll_wait()`立即感知到，服务器随即调用`ngx_close_connection()`立即回收了。
-   如果第7秒的时候，客户端B连接，新链接进入，系统会把刚才A用过的连接池中的连接分配给B。
-   此时10秒到了，A的执行函数返回的时候，这个线程很可能会继续操作连接「修改读数据」，此时修改的是B的连接。很可能导致服务器崩溃。 



如果业务很复杂，很有可能连接池中的连接会保存很多数据。



>   一个连接，如果程序判断这个连接不用了，那么不应该把这个 连接立即放到空闲队列中，而是放到某个地方等待一段时间（60S），60秒之后，再真正回收这个连接到 「连接池/空闲队列」中去，这种连接才可以真正的分配给其他用户使用。
>
>   **为什么要等待60秒？**
>
>   就是确保 客户端A真掉线了，那我我执行的该用户的业务逻辑也一定能在这个等待时间内全部完成。

所以连接不立即回收是非常重要的，有个时间缓冲非常重要，这个可以极大确保服务器的稳定。

官方nginx会立即回收，它必须确保过期的包不用连接池。



这种回收方法称为「延迟回收」，所以需要优化代码了。众所周知服务器程序需要常年累月的优化。



#### 7.4.2 灵活创建连接池

在`ngx_epoll_init()`中调用`initconnection()`，先创建10000个连接，如果日后不够，则再继续添加。针对每个连接分配一段内存，每个连接池中的对象都有一个互斥量。

`GetOneToUse()`函数代表每个元素复用的时候，需要一个函数把一些数据修改，`iCurrsequence`要++。

在连接池中引入：

```cpp
std::list<lpngx_connection_t>  m_connectionList;                      //连接列表【连接池】
std::list<lpngx_connection_t>  m_freeconnectionList;                  //空闲连接列表【这里边装的全是空闲的连接】
std::atomic<int>               m_total_connection_n;                  //连接池总连接数
std::atomic<int>               m_free_connection_n;                   //连接池空闲连接数
```

初始化的时候，需要把每个连接都放入到这两个列表中。

`ngx_get_connection`先判断有没有空闲链接，如果没有新创建一个连接，这个连接只能放到「连接链表」而不能放到「空闲链表」。

`clearconnection()`回收连接池，每次从「连接链表」中取一个元素，手动调用析构函数，并释放内存。

#### 7.4.3 连接池中连接的回收

-   立即回收
    -   `accept()`用户没有接入时可以立即回收，因为刚连入进来，还没有进行数据收发
    -   添加连接的读事件失败
-   延迟回收的场景
    -   用户接入进来，线程开始干活
    -   如果开启踢人时钟，则检查心跳包的时候就直接踢出（延迟回收）
    -   长时间没发送心跳包，踢出
    -   洪水攻击，直接回收
    -   recv返回值为0，回收
    -   recv返回值\<0，同时errno不是`EAGAIN`、`EINTER`等
    -   连接中发送数据的条数太多，直接踢出



#### 7.4.4 立即回收

`ngx_free_connection()`函数用于立即回收：

1.   先加锁
2.   回收前调用`PutOneToFree()`
3.   放入空闲链表
4.   空闲链表数++



#### 7.3.5 延迟回收

`inRecyConnectQueue()`函数。可能在收包的时候，用户断开了，需要调用「延迟回收」。

延迟回收又定义了一个互斥量`m_recyconnqueueMutex`(因为有一个专门的线程也要用到延迟回收链表）和延迟回收的链表。

```cpp
pthread_mutex_t                m_recyconnqueueMutex;                  //连接回收队列相关的互斥量
std::list<lpngx_connection_t>  m_recyconnectionList;                  //将要释放的连接放这里
```



1.    打印日志
2.   加锁
3.   `iCurrseqence`++，记录当前回收的时间
4.   放入延迟回收列表
5.   延迟回收列表元素数++

同时引入一个配置项：

```c
#Sock_RecyConnectionWaitTime:为确保系统稳定socket关闭后资源不会立即收回，而要等一定的秒数，在这个秒数之后，才进行资源/连接的回收
Sock_RecyConnectionWaitTime = 80
```

也就是延迟回收的等待秒数。



**同时，使用一个新的线程来单独处理延迟回收。**

这个线程的入口函数为：`void* CSocket::ServerRecyConnectionThread(void* threadData)`，在`Initialize_subproc()`被调用。

这个线程的死循环中行为：

1.   休息200ms

2.   判断回收队列是否\>0，大于0则进行下一步

3.   加锁

4.   判断是否超过时间，

5.   如果到时间了，就调用`ngx_free_connection()`立即回收。

     或者程序要退出了，也会调用`ngx_free_connectin()`硬释放。

做一些判断，



#### 7.3.6 延迟回收的具体应用

`recvproc()`的延迟回收取代了立即回收。`m_ifTimeOutKick`代表踢人时钟，`m_ifkickTimeCount`代表是否开启心跳包 

`Initializa_subproc()`中放了一些子进程中才干的事情（主要是多线程相关的信息）：

-   发消息互斥量初始化
-   连接相关互斥量初始化
-   连接回收相关队的初始化
-   连接回收队列相关互斥量初始化
-   。。。



它在`ngx_worker_process_init()`子进程初始化中被调用。

`Shutdown_subproc()`进行一些子进程的关闭退出函数，在子进程的for循环之后调用（所以暂时执行不到这里）。

先处理消息队列，再处理多线程。

### 7.5 程序退出时线程的安全终止

可能在以后，还会引入一些新的线程来干别的工作。

官方nginx内存没释放，因为太复杂了，但是不太「优雅」。



线程完美退出。

在`Shutdown_subproc()`函数中，先关闭所有的工作线程。

```cpp
void CSocket::Shutdown_subproc()
{
    //(1)把干活的线程停止掉，注意 系统应该尝试通过设置 g_stopEvent = 1来 开始让整个项目停止
    std::vector<ThreadItem*>::iterator iter;
	for(iter = m_threadVector.begin(); iter != m_threadVector.end(); iter++)
    {
        pthread_join((*iter)->_Handle, NULL); //等待一个线程终止
    }
    //(2)释放一下new出来的ThreadItem【线程池中的线程】    
	for(iter = m_threadVector.begin(); iter != m_threadVector.end(); iter++)
	{
		if(*iter)
			delete *iter;
	}
	m_threadVector.clear();

    //(3)队列相关
    clearMsgSendQueue();
    clearconnection();
    
    //(4)多线程相关    
    pthread_mutex_destroy(&m_connectionMutex);          //连接相关互斥量释放
    pthread_mutex_destroy(&m_sendMessageQueueMutex);    //发消息互斥量释放    
    pthread_mutex_destroy(&m_recyconnqueueMutex);       //连接回收队列相关的互斥量释放
    sem_destroy(&m_semEventSendQueue);                  //发消息相关线程信号量释放
}
```



找一个合适的地方，把`g_stopEvent`设置为1，并调用`Shutdown_subproc()`函数。但是具体触发什么条件，需要思考，在`ngx_worker_process_cycle()`中可以补充。



>   似乎可以通过捕获信号量。



### 7.6 epoll事件的改造

#### 7.6.1 增加新的事件处理函数

事件可以控制的更加精细。

引入一个新的事件处理函数`ngx_epoll_oper_event()`，原理和`epoll_add_event()`差不多，都调用`epoll_ctl()`函数。



>   代码总是越写越清晰。



同时对于监听端口，调用这个函数；连接端口也调用这个函数。

由于`iCurrsequence`的存在，不会存在B连接使用A连接的信息的情况。官方用的是`instance`。



#### 7.6.2 测试

打两个日志。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230311163650070.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230311163827673.png" alt="image-20230311163827673" style="zoom:50%;" />



### 7.7 深入了解LT发数据机制

在水平触发模式下，发送数据有哪些注意事项。

通过调用`ngx_epoll_oper_event()`函数修改，那么当socket可写时，可以触发socket的可写事件，得到这个事件就可以发送数据了。

>   **一个问题**：什么叫「可写」？
>
>   每一个TCP连接（socket），三次握手之后，服务器端都会有一个接收缓冲区和一个发送缓冲区，这些缓冲区都有一个缺省的大小，发送缓冲区缺省大小一般为10几k，接收缓冲区大概有几十k。可以通过`setsockopt()`来设置。
>
>   socket可写就是针对发送缓冲区而言的。当调用`send()/write()`发送数据的时候，实际上这两个函数是把数据放到了数据缓冲区，放到缓冲区之后，`send()/write()`就返回了。这就意味着这两个函数执行成功并不是真正发送出去了，内核会尝试把客户缓冲区的数据发送，对方用`read()/recv()`的时候会真正发送出去，当客户端发过来一个明确的信息表示自己已经收到一部分数据了，服务端才会清空这一部分在发送缓冲区的数据。



所以客户端必须及时调用`read()/recv()`把数据取走，如果客户端接收数据很慢，就会造成发送端接收数据太快。如果把缓冲区填满，服务端再次调用`send()/write()`函数，就会返回`EAGAIN`错误。这个错误本质上不是一个错误，而是一个提示。

当socket可写的时候（发送缓冲区没满），会触发socket可写事件，问题在于LT模式**会不停地触发**socket可写事件。

有两种解决方案：

#### 解决方法1

需要向socket写数据的时候把socket事件加入到epoll中，等待可写事件，当可写事件来时，系统会通知，此时可以调用`write()`或`send()`发送数据，当发送数据完毕后，把`socket`的写事件通知，从红黑树中移除（注意移除的是写事件通知，而不是节点）。

这种方式存在缺点：

-   即使发送很少的数据，也需要把事件通知加入到epoll中，写完毕后又需要把写事件通知从红黑树中移除，对于效率有一定的影响（有一定的操作代价）



#### 解决方案2

开始不把socket写事件通知加入到epoll，当需要写数据的时候，直接调用`write()/send()`直接发送数据。

-   这种方式缓冲区没满就会发送
-   如果满了就返回`EAGAIN`，需要等待可写事件，此时判断`==EAGAIN`加入到`epoll`中，就变成在epoll驱动下写数据，有写事件发生才发送数据。发送完数据再移除写事件。



这种方式的优点：

-   数据不多的时候，可以避免epoll写事件的增加和删除，提高程序执行效率

本项目采用这种方法发送数据。



### 7.8 发数据、信号量、并发、多线程综合实战

#### 7.8.1 发送数据指导思想和信号量

要发送的数据放到一个队列中，专门创建一个线程来统一负责数据发送，而不是在主线程中发送。

使用`msgSend()`将要发送的数据增加到 发送数据的队列中去。





>   信号量和互斥量
>
>   互斥量就是`pthread_mutex_t`类型，信号量也是一种线程之间的同步机制，是`sem_t`类型。
>
>   -   互斥量一般用于线程之间同步
>   -   信号量：能够提供进程之间同步和进程之间的同步
>
>   本项目使用信号量进行线程同步。
>
>   《Unix网络编程 卷2》第二版第十章有信号量的用法。

用之前需要初始化`sem_init()`：

```cpp
//初始化发消息相关信号量，信号量用于进程/线程 之间的同步，虽然 互斥量[pthread_mutex_lock]和 条件变量[pthread_cond_wait]都是线程之间的同步手段，但
//这里用信号量实现 则 更容易理解，更容易简化问题，使用书写的代码短小且清晰；
//第二个参数=0，表示信号量在线程之间共享，确实如此 ，如果非0，表示在进程之间共享
//第三个参数=0，表示信号量的初始值，为0时，调用sem_wait()就会卡在那里卡着
if(sem_init(&m_semEventSendQueue,0,0) == -1)
{
    ngx_log_stderr(0,"CSocket::Initialize()中sem_init(&m_semEventSendQueue,0,0)失败.");
    return false;
}
```

用完之后`sem_destroy()`销毁。



`sem_wait()`和`sem_post()`用到了信号量。

-   `sem_wait()`用来测试指定信号量的值
    -   如果该值`>0`，那么将该值`-1`并立即返回。
    -   如果该值`=0`，那么该线程将进入睡眠，直到信号量的值`>0`，将值-1并返回。
-   `sem_post()`将信号量的值+1，哪怕没有线程在等待该信号量。这点与互斥量不同。

#### 7.8.2 发送数据代码

`msgSend()`函数:

```cpp
//将一个待发送消息入到发消息队列中
void CSocket::msgSend(char *psendbuf) 
{
    CLock lock(&m_sendMessageQueueMutex);  //互斥量
    m_MsgSendQueue.push_back(psendbuf);    
    ++m_iSendMsgQueueCount;   //原子操作

    //将信号量的值+1,这样其他卡在sem_wait的就可以走下去
    if(sem_post(&m_semEventSendQueue)==-1)  //让ServerSendQueueThread()流程走下来干活
    {
         ngx_log_stderr(0,"CSocket::msgSend()中sem_post(&m_semEventSendQueue)失败.");      
    }
    return;
}
```

1.   多线程对于数据必须互斥，自动释放。
2.   同时使用`m_iSendMsgQueueCount`原子操作计数，使用`size()`可能会慢一点。原子操作只能`++`，不能`+1`。
3.   调用`sem_post()`，信号量值+1



#### 7.8.3 数据发送线程

在subproc的初始化中创建线程：

```cpp
err = pthread_create(&pSendQueue->_Handle, NULL, ServerSendQueueThread,pSendQueue); //创建线程，错误不返回到errno，一般返回错误码
if(err != 0)
{
    return false;
}
```

在`ServerSendQueueThread()`函数中，临界一次，就把消息队列中所有的数据都发送。

#### 7.8.4 可写通知到达后数据的继续发送

如果发送缓冲区满了，证明发送的数据并不完整，当可写通知来了之后，需要继续处理数据的发送。

`ngx_write_request_handler`函数



#### 7.8.5 测试发送数据

客户端故意不接受包，客户端每次收取一个包。需要**发送非常多次 65k左右的数据，**才会收到发送缓冲区满。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230312204852318.png" alt="image-20230312204852318" style="zoom:50%;" />

红线就是epoll机制下的发送消息，客户端收了**一个包**，然后客户端继续发包，还是需要连续发送很多次请求，才会缓冲区满。



通过测试，得到结论：

-   `ngx_write_request_handler()`的执行逻辑应该是正确的，能够把剩余的未成功发送的数据发送出去
-   LT模式下，发送数据采用的改进方案也是有效的。
-   发送缓冲区只有几十k，测试可以发送1M大小才满
-   所以数据可能先被发送到了客户端的接收缓冲区

所以不要以为发送缓冲区有多大，只`send()`就能把缓冲区填满。不管怎么说，只要对方不接受数据，发送缓冲区就会满。



#### 7.8.6 发送数据后续处理代码

到这里发送数据的代码就差不多完成了。



## 8. 服务器安全与完善

### 8.1 小总结

**实现的功能：**

1.   服务器按照包头包体格式正确接受客户端发送过来的数据包
2.   根据收到的包的不同，来执行不同的业务处理逻辑
3.   把业务处理产生的结果数据包发送给客户端



**主要技术：**

1.   epoll高并发通讯技术
2.   线程池技术来处理业务逻辑
3.   线程之间的同步技术，包括互斥量、信号量
4.   信号、日志打印、fork创建子进程，守护进程创建



**借鉴了哪些官方nginx：**

1.   「一个master进程，多个worker子进程」的进程框架
2.   借鉴了epoll的一些实现代码，但是本项目使用LT模式，官方采用的是ET模式
3.   借鉴了接受数据包，以及发送数据包的核心代码
4.   创建一个函数数组来处理不同业务



**nginx还有的好的功能：**

1.   nginx热更新
2.   worker子进程挂了master能够重新启动worker
3.   重载配置文件



**没有借鉴官方nginx代码（也是本项目的核心代码）：**

1.   epoll技术采用LT模式来实现网络数据的收发
2.   写了一套线程池来处理业务逻辑，调用适当的业务处理逻辑函数，直至处理完毕把数据发送回客户端
3.   实现了一个连接池，采用延迟回收技术，以及专门处理数据发送的数据线程



### 8.2 心跳包

#### 8.2.1 心跳包的概念

无论是客户端还是服务端都有责任把心跳包机制实现好，以确保程序能良好的工作，应付意外情况发生。

「心跳包」就是一个普通的数据包，一般每隔几十秒（10秒到1分钟之间），由客户端主动发送给服务器，服务器收到之后，服务端也会给客户端发送一个心跳包。如此循环，直到客户端关闭。

三次握手完成，TCP连接建立之后，才存在发送心跳包的问题。

如果客户端不给服务端发送心跳包，服务器会怎样：

-   一般来说，服务器会 在间隔3倍时间间隔到来时，主动关闭socket连接。

作为一个客户端程序，应该：

-   如果发送了心跳包给服务器，但是在 3倍时间间隔到来时，客户端也应该主动关闭socket连接
-   如果业务需要重连，客户端程序在关闭连接后，还要主动再次尝试连接服务器
-   客户端程序也有必要提示使用者「与服务器的连接已经断开」



#### 8.2.2 为什么引入心跳包

常规客户端关闭，服务端能感知到。

TCP协议本身有`keep-alive`机制，可以判断双方是否断线，缺省是关闭的。

有一种特殊情况，连接断开，客户端和服务器端都感知不到：

1.   C/S程序运行在不同的两台物理电脑上，TCP已经建立
2.   拔掉C/S的网线，服务端是感知不到客户端断开的，双方都感知不到！（这才是心跳包存在的重要原因）

为了应对拔网线，导致不知道对方是否断开了TCP连接这种事，引入了心跳包（拔网线）。

超时没有发送来心跳包，那么就会将对端的socket连接close掉，回收资源，就是心跳包的作用。

其他作用：

-   检测网络延迟等。

本项目主要检测双方连接是否断开。



#### 8.2.3 接收心跳包与返回结果的代码

心跳包（ping包）

心跳包规定消息代码为0，心跳包也不需要包体，只有包头就够了，引入一个新的函数`_HandlePing()`来处理，放在`ngx_c_logic.h`中，并在成员函数数组中修改这么一项。

```cpp
//接收并处理客户端发送过来的ping包
bool CLogicSocket::_HandlePing(lpngx_connection_t pConn,LPSTRUC_MSG_HEADER pMsgHeader,char *pPkgBody,unsigned short iBodyLength)
{
    //心跳包要求没有包体；
    if(iBodyLength != 0)  //有包体则认为是 非法包
		return false; 

    CLock lock(&pConn->logicPorcMutex); //凡是和本用户有关的访问都考虑用互斥，以免该用户同时发送过来两个命令达到各种作弊目的
    pConn->lastPingTime = time(NULL);   //更新该变量

    //服务器也发送 一个只有包头的数据包给客户端，作为返回的数据
    SendNoBodyPkgToClient(pMsgHeader,_CMD_PING);

    ngx_log_stderr(0,"成功收到了心跳包并返回结果！");
    return true;
}
```

和本用户有关的都互斥，防止两个命令同时到达。同时引入`lastPingTime`用来记录最后一个心跳包的事件。



#### 8.2.4 处理不发送心跳包的客户端

如果客户端不及时发送心跳包中来，设置一个阈值（例如100秒），超过这个时间依然没收到心跳包，即i那么服务器端就把TCP断开。

首先在配置文件中添加两个配置：

```
#Sock_WaitTimeEnable：是否开启踢人时钟，1：开启   0：不开启
Sock_WaitTimeEnable = 1
#多少秒检测一次是否 心跳超时，只有当Sock_WaitTimeEnable = 1时，本项才有用，<5会默认改成5
Sock_MaxWaitTime = 20
```

为了测试方便改为20秒，代码中阈值为(3 * 20 + 10)秒。



新建立一个`ngx_c_socket_time.cxx`用来存放一些时间相关的函数，并在`ngx_event_accept()`中判断是否调用`AddToTimerQueue()`，（在连接建立时调用，并且`Sock_Wait_TimeEnalbe`打开）。`AddToTimerQueue()`的作用是「把一个连接信息加入到时间队列中来」，同时这个函数还会更新最早的连接。

**定时器队列：**

-   nginx官方采用红黑树
-   网上还有时间轮
-   本项目使用`multimap`来制作定时器，实际上也是红黑树



```cpp
//设置踢出时钟(向multimap表中增加内容)，用户三次握手成功连入，然后我们开启了踢人开关
//【Sock_WaitTimeEnable = 1】，那么本函数被调用；
void CSocket::AddToTimerQueue(lpngx_connection_t pConn)
{
    CMemory *p_memory = CMemory::GetInstance();

    time_t futtime = time(NULL);
    futtime += m_iWaitTime;  //当前时间+等待时间，就是20秒之后的时间

    CLock lock(&m_timequeueMutex); //互斥，因为要操作m_timeQueuemap了
    LPSTRUC_MSG_HEADER tmpMsgHeader = (LPSTRUC_MSG_HEADER)p_memory->AllocMemory(m_iLenMsgHeader,false);
    tmpMsgHeader->pConn = pConn;
    tmpMsgHeader->iCurrsequence = pConn->iCurrsequence;
    m_timerQueuemap.insert(std::make_pair(futtime,tmpMsgHeader)); //按键 自动排序 小->大
    m_cur_size_++;  //计时队列尺寸+1
    m_timer_value_ = GetEarliestTime(); //计时队列头部时间值保存到m_timer_value_里
    return;    
}
```

消息队列`m_timerQueuemap`的key是`time_t`，值是「消息头」，消息头中有连接的指针和`iCurrsequence`。

`GetOverTimeTimer()`函数 调用`RemoveFirstTimer()`函数,`RemoveFirstTimer`函数负责：

-   取出一个元素的`second`元素，并erase这个元素
-   时间队列\-\-
-   返回取出的元素

`procPingTimeOutChecking()`用来检测心跳包发送，也就是判断`lastPingTime > 给定时间 * 3 +10`秒

每20秒看一次（决定是否踢出去），那么需要添加一个，同时添加一个时间相关的互斥量，并在主函数中初始化一下。



**谁来处理时间队列中的数据？**

创建一个新线程，专门处理时间队列。

新线程的入口函数`ServerTimerQueueMonitorThread()`。

时间队列监视和处理线程，处理到期不发心跳包的用户踢出的线程。在死循环中，判断是否有超时项存在，如果存在，就断开。

`procPingTimeOutChecking()`，心跳时间到了，检查是否发送心跳包过来。

`zdClosesocketProc()`关闭一个socket时的善后处理函数



同时有可能同一个连接被关闭两次；还有可能正准备close某个连接的时候，对方正好断开，这个连接的socket句柄就被另一个客户连接。



**测试：**

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230313001254587.png" alt="image-20230313001254587" style="zoom:50%;" />



### 8.3 控制并发连入数量



#### 8.3.1 统计用户连入数

epoll就是高并发的代名词，听起来epoll特别厉害，动不动就是数十万百万并发，但是在实际业务中，需要辩证看待，如果每个连接都在收发数据，实际并发不会特别高。

单纯探讨epoll能支持多少连接，意义并不大。

也就是说并发数量取决于很多因素：

-   采用的开发技术（epoll支持数十万并发）
-   服务器收发数据的频繁程度以及具体要处理的业务的复杂程度，如果业务太复杂，占用的系统资源可能很多，也会 拖累数据的收发
-   实际的物理内存，可用的物理内存数量会直接决定支持的并发连接
-   一些其他TCP/IP配置项



一般来说，一台机器支持1～2万的并发基本上就够用了。

一个服务器程序，需要根据具体内存，以及具体要实现的业务等因素，控制能够同时连入的客户端数量，如果允许客户端无限连入，那么服务器一定会崩溃。

需要限制当前在线用户数量，`m_onlineUserCount`用来记录当前在线的用户数量。用户连入成功后这个值`+1`，在`inRecyConnectQueue`中在线用户连入`-1`。



#### 8.3.2 控制用户连入解决思路

如果同时连入的用户数量 超过了 「允许的最大连入数量」时，直接把这个连入的用户直接踢出去。

允许最大连接数量就是`m_worker_connections`，





### 8.4 黑客攻击的防范

#### 8.4.1 攻击效果

轻则：服务器工作延迟，效率明显降低

重则：整个服务器完全停摆，没有办法提供正常服务，例如拒绝服务攻击就会导致这种状况（服务器失去响应）

最甚者：如果服务器程序有一些漏洞的话，恶意黑客很有可能利用给一些远程溢出攻击手段直接攻破 服务器程序所在的计算机，能够拿到一定的权限，甚至是root权限，一旦拿到了root权限，那么整个计算机都在黑客控制中了。



有些攻击是利用TCP/IP协议 先天的一些设计问题来攻击，例如 SYN flood攻击。

DDoS攻击，分布式拒绝服务攻击（SYN flood是DDoS攻击的一种），甚至防火墙等设备都可能防不住，甚至得从数据路由器想办法。

本项目对于DDoS攻击是没有办法解决的，但是可以解决一些用户三路握手连入进来后的一些网络安全问题。

#### 8.4.2 flood攻击防范及代码

以游戏服务器为例，一般一秒不会超过10个数据包（手按一下）。

假设我们认为一个合理的客户端一秒钟发送给服务器不超过10个。如果有一个客户端不停地给服务器发送数据包，一秒钟超过了10个数据包，那么服务器就认为这个客户有恶意攻击的倾向，那么服务器就应该果断把这个TCP客户端连接关闭，也是服务器发现恶意玩家以及保护自己安全的手段。



**代码实现**

需要引入一些配置项：

```
#和网络安全相关
[NetSecurity]

#flood检测
#Flood攻击检测是否开启,1：开启   0：不开启
Sock_FloodAttackKickEnable = 1
#Sock_FloodTimeInterval表示每次收到数据包的时间间隔是100(单位：毫秒)
Sock_FloodTimeInterval = 100
#Sock_FloodKickCounter表示计算到连续10次，每次100毫秒时间间隔内发包，就算恶意入侵，把他kick出去
Sock_FloodKickCounter = 10
```



引入`TestFlood()`来判断是否有 Flood攻击，**每次收到完整的包都调用**`TestFlood()`（这个函数在处理消息之前），返回一个`isflood`bool类型参数。

一旦有flood倾向，最终`ngx_read_request_handler()`会感知到，并把它放入关闭消息队列中。



**简单测试：**

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230313131544814.png" alt="image-20230313131544814" style="zoom:50%;" />

可以看到，客户端连续发送10次包后， 服务器干掉了客户端。



#### 8.4.3 畸形数据包防范

>   客户端发送过来的数据并不可靠。

这些数据包可能是造假 或者 畸形，服务器端必须判断数据包合理合法。

以游戏服务器为例：

1.   数据造假，100金币发出了要买10000金币装备的 数据包，或者要买 负数 的装备
     -   解决方法，服务器端多做判断，基本能够避免客户端造假
2.   畸形数据包，有一种黑客攻击叫 远程溢出攻击， 能够成功的主要原因是服务器程序书写不当，比如接收到的数据缺少边界检查



以`_HandlerRegister()`为例：

包体为：

```cpp
typedef struct _STRUCT_REGISTER
{
	int           iType;          //类型
	char          username[56];   //用户名 
	char          password[40];   //密码

}STRUCT_REGISTER, *LPSTRUCT_REGISTER;
```

**我们有必要在字符数组末尾，自己手动增加一个`\0`，就可以确保使用这个字符串（字符数组）不会出问题：**

```cpp
p_RecvInfo->username[sizeof(p_RecvInfo->username)-1]=0;//这非常关键，防止客户端发送过来畸形包，导致服务器直接使用这个数据出现错误。 
p_RecvInfo->password[sizeof(p_RecvInfo->password)-1]=0;//这非常关键，防止客户端发送过来畸形包，导致服务器直接使用这个数据出现错误。 
```

`\0`对应的ascii就是0。不管字符串是多少，最后一个都是`\0`，为了防止有人恶意在字符串中不加`\0`。



#### 8.4.4 超时直接踢出服务器

账号服务器：

-   有一个需求，超过20秒不主动断开与本服务器连接的，那么本服务器就要主动把这个用户踢下线（断开TCP连接）。

每一个连接都会占用资源，句柄、内存等。

配置文件增加一项：

```
#当时间到达Sock_MaxWaitTime指定的时间时，直接把客户端踢出去，只有当Sock_WaitTimeEnable = 1时，本项才有用
Sock_TimeOutKick = 0
```

调整`GetOverTimeTimer()`，`procPingTimeOutChecking()`函数中到时间就直接踢出去，否则再去判断心跳包是否收到了。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230313135943259.png" alt="image-20230313135943259" style="zoom:50%;" />

心跳包时间设置短一点，直接提出。

这个开关需要根据实际需求决定是否打开。



### 8.5 输出一些观察信息

每隔10秒钟，把一些关键信息显示在屏幕上：

-   当前在线人数
-   和连接池有关的：连接列表大小，空闲连接列表大小，要释放的连接有多少个
-   当前时间队列，也就是要踢出的
-   收消息队列和发消息队列的大小

引入一个`printTDInfo()`函数，每10秒打印一次，如果消息队列太多，可以提示一下。

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230313144032545.png" alt="image-20230313144032545" style="zoom:50%;" />

这个函数在`ngx_process_events_and_timers()`，也就是子进程的主循环中。



### 8.6 遗漏的安全问题

#### 8.6.1 收到了太多数据包处理不过来

限速：epoll技术，一个限速的思路：

-   在epoll红黑树节点中，把`EPOLLIN`可读通知移除
-   等到速度正常的时候，再把`EPOLLIN`加入。

本项目，在`printTDInfo()`中做了一个简单提示。



#### 8.6.2 积压太多数据包发送不出去

` CSocekt::msgSend(char *psendbuf) `函数中的处理：

```cpp
//发送消息队列过大也可能给服务器带来风险
if(m_iSendMsgQueueCount > 50000)
{
    //发送队列过大，比如客户端恶意不接受数据，就会导致这个队列越来越大
    //那么可以考虑为了服务器安全，干掉一些数据的发送，虽然有可能导致客户端出现问题，但总比服务器不稳定要好很多
    m_iDiscardSendPkgCount++;
    p_memory->FreeMemory(psendbuf);
    return;
}
//总体数据并无风险，不会导致服务器崩溃，要看看个体数据，找一下恶意者了    
LPSTRUC_MSG_HEADER pMsgHeader = (LPSTRUC_MSG_HEADER)psendbuf;
lpngx_connection_t p_Conn = pMsgHeader->pConn;
if(p_Conn->iSendCount > 400)
{
    //该用户收消息太慢【或者干脆不收消息】，累积的该用户的发送队列中有的数据条目数过大，认为是恶意用户，直接切断
    ngx_log_stderr(0,"CSocekt::msgSend()中发现某用户%d积压了大量待发送数据包，切断与他的连接！",p_Conn->fd);      
    m_iDiscardSendPkgCount++;
    p_memory->FreeMemory(psendbuf);
    zdClosesocketProc(p_Conn); //直接关闭
    return;
}

```

直接把要发送的数据包丢掉了，并在打印数据函数中打印。

如果有用户只发包不收包（累计发送队列中有 超过400条数据），认为是恶意用户，直接断开。



#### 8.6.3 连入安全的进一步完善

如果用户连接数过多，如果还有用户连入，就会`new`，因为本服务器是延迟回收，所有有可能连接远大于连接池的数量。

```cpp
//如果某些恶意用户连上来发了1条数据就断，不断连接，会导致频繁调用ngx_get_connection()使用我们短时间内产生大量连接，危及本服务器安全
if(m_connectionList.size() > (m_worker_connections * 5))
{
    //比如你允许同时最大2048个连接，但连接池却有了 2048*5这么大的容量，这肯定是表示短时间内 产生大量连接/断开，因为我们的延迟回收机制，这里连接还在垃圾池里没有被回收
    if(m_freeconnectionList.size() < m_worker_connections)
    {
        //整个连接池这么大了，而空闲连接却这么少了，所以我认为是  短时间内 产生大量连接，发一个包后就断开，我们不可能让这种情况持续发生，所以必须断开新入用户的连接
        //一直到m_freeconnectionList变得足够大【连接池中连接被回收的足够多】
        close(s);
        return ;   
    }
}
```



### 8.7 压力测试前的准备工作

#### 8.7.1 配置文件内容和配置项确认

压力测试时采用的配置文件：

```

#是注释行
#每个有效配置项用 等号 处理，等号前不超过40个字符，等号后不超过400个字符；

 
#[开头的表示组信息，也等价于注释行
[Socket]

#监听端口
ListenPort = 5678    

#DBInfo代表数据信息
DBInfo = 127.0.0.1;1234;myr;123456;mxdb_g

#日志相关
[log]
# 日志文件输出目录和文件名
# Log=logs/error.log
Log = logs/error.log

# 打印日志的最低等级(<= 数字)到日志文件中
# 测试时可以使用8, 运行时可以用 4
LogLevel = 8

#进程相关
[Proc]
#创建 这些个 worker进程
WorkerProcesses = 1

#是否按守护进程方式运行，1：按守护进程方式运行，0：不按守护进程方式运行
Daemon = 1

#处理接收到的消息的线程池中线程数量，不建议超过300
ProcMsgRecvWorkThreadCount = 120

# 网络相关
[Net]
# 一般监听一个端口就够了, 有的web服务器监听80和443端口
ListenPortCount = 1
# 监听的两个端口, 
ListenPort0 = 80
# ListenPort1 = 443

#epoll连接的最大数【是每个worker进程允许连接的客户端数】，实际其中有一些连接要被监听socket使用，实际允许的客户端连接数会比这个数小一些, 10w并发就填100000
worker_connections = 2048
#Sock_RecyConnectionWaitTime:为确保系统稳定socket关闭后资源不会立即收回，而要等一定的秒数，在这个秒数之后，才进行资源/连接的回收
Sock_RecyConnectionWaitTime = 150

#Sock_WaitTimeEnable：是否开启踢人时钟，1：开启   0：不开启
Sock_WaitTimeEnable = 1
#多少秒检测一次是否 心跳超时，只有当Sock_WaitTimeEnable = 1时，本项才有用
Sock_MaxWaitTime = 20

#当时间到达Sock_MaxWaitTime指定的时间时，直接把客户端踢出去，只有当Sock_WaitTimeEnable = 1时，本项才有用
Sock_TimeOutKick = 0

#和网络安全相关
[NetSecurity]

#flood检测
#Flood攻击检测是否开启,1：开启   0：不开启
Sock_FloodAttackKickEnable = 1
#Sock_FloodTimeInterval表示每次收到数据包的时间间隔是100(单位：毫秒)
Sock_FloodTimeInterval = 100
#Sock_FloodKickCounter表示计算到连续10次，每次100毫秒时间间隔内发包，就算恶意入侵，把他kick出去
Sock_FloodKickCounter = 10
```



#### 8.7.2 整理业务处理逻辑

把打印都关掉，只留下一个打印函数。



### 8.8 压力测试

<img src="https://raw.githubusercontent.com/Missyesterday/picgo/main/picgo/image-20230313155606798.png" alt="image-20230313155606798" style="zoom:50%;" />

主要测试：

-   程序崩溃
-   程序异常
-   服务器程序占用内存不断增加

`cat /proc/26420/status`和`htop -p 26420`可以查看对应pid的资源占用情况。

>   为什么`accept4()`失败？
>
>   这个和用户进程可以打开的文件数量有关，因为系统为每个tcp连接要创建一个socket句柄，每个socket句柄也是一个文件句柄
>
>   `ulimit -n`可以查看每个进程允许打开的文件数。

需要修改Linux对当前用户进程 同时打开文件数量的限制。

`ulimit -n 65535`就可以修改最大文件描述符为65535。



### 8.9 惊群

#### 8.9.1 CPU占比与惊群

`top`和`htop`都能看运行时CPU占比。

`top`命令并不是一个简单的命令，如果CPU有四个核，那么每个核都有100%可以用。



#### 8.9.2 惊群

**惊群：**

-   一个master进程，多个worker进程
-   在master进程中就开始监听端口，就导致每个worker子进程都监听了端口
-   惊群就是指「一个连接进入，惊动了多个worker进程，但是只有一个worker进程`accept()`,其他三个worker进程被惊动。
-   这三个被惊动的worker进程都做了无用功，还导致系统调度。这是操作系统的本身的缺陷。

nginx的解决方法是：

-   进程之间加锁，谁得到这个锁，就增加可读标记。

随着操作系统的升级，内核版本3.9以上的Linux版本，解决了惊群问题，据说性能比官方nginx的解决办法要高很多。

>   内核是如何解决惊群问题的？
>
>   复用端口，是一种套接字的复用机制，允许将多个套接字`bind`到同一个ip地址/端口上，这样以来，就可以建立多个服务器来接收到同一个端口的连接。也就是多个worker进程能监听同一个端口。



**reuseport的代码。**

```cpp
//为处理惊群问题使用reuseport
//setsockopt（）:设置一些套接字参数选项；
//参数2：是表示级别，和参数3配套使用，也就是说，参数3如果确定了，参数2就确定了;
//参数3：允许重用本地地址
//设置 SO_REUSEADDR，目的第五章第三节讲解的非常清楚：主要是解决TIME_WAIT这个状态导致bind()失败的问题
int reuseaddr = 1;  //1:打开对应的设置项
if(setsockopt(isock,SOL_SOCKET, SO_REUSEADDR,(const void *) &reuseaddr, sizeof(reuseaddr)) == -1)
{
    ngx_log_stderr(errno,"CSocket::Initialize()中setsockopt(SO_REUSEADDR)失败,i=%d.",i);
    close(isock); //无需理会是否正常执行了                                                  
    return false;
}
```

-   很多socket配置项可以通过`setsockopt()`函数来配置。
-   还有一些tcp/ip协议的配置项，可以通过修改配置文件来生效



>   把bind端口放到worker进程创建，能不能解决惊群问题呢？
>
>   如果在master进程中调用`ngx_open_listening_socket()`函数，建议在 master进程中把监听socket关闭



### 8.10 性能优化大局观

-   性能优化没有尽头
-   没有一个标准的优化方法，只能依据具体情况而定



可以从两个层面看性能优化：

-   **软件层面：**
    -   充分利用CPU，例如解决惊群问题
    -   深入了解tcp/ip协议，通过一些协议参数配置来进一步改善性能
    -   处理业务逻辑方面，算法方面的内容，可以提前做好
-   **硬件层面：**
    -   高速网卡，增加网络带宽
    -   专业服务器
    -   内存：容量大、访问速度快
    -   主板，总线的优化



### 8.11 性能优化的实施

#### 8.11.1 绑定CPU、提升进程优先级

**绑定CPU**

创建worker进程的时候把进程绑定到CPU的一个核心上，那么就不存在所谓「进程切换」问题。

为什么「一个worker进程运行在一个核上」，能提升效率？

CPU：是有缓存的，能极大提升运行效率，如果一个进程固定到某个CPU核上，能大大提高CPU的缓存命中率。

在官方nginx中，有一个配置项`worker_cpu_affinity`（CPU亲和性），就是为了把worker进程固定地绑定到某个cpu核上，函数是`ngx_setaffinity()`。

https://github.com/SmartKeyerror/reading-source-code-of-nginx-1.19.10

https://www.cnblogs.com/dragonsuc/p/5512797.html



**提升进程优先级**

CPU把运行时间分成很多时间片，每一个时间片运行一个进程，可以提高某个进程的优先级，那么这个进程就有机会被分配到更多的CPU时间（CPU时间片，上下文切换问题），得到执行的机会就会增加。

系统调用`setpriority()`可以设置进程优先级。



正常来说，进程工作的时候处于`R`状态，等待事件的时候处于`S`状态。



`pidstat`命令可以查看上下问切换次数。`-w -p pid`。切换频率越低越好：

-   `cswch/s`：主动切换/秒，本来时间片是该进程的，但是在等待，让出了自己的时间片
-   `nvcswch/s`：被动切换/秒，CPU调度

所以测试的时候最好在两台物理机器上。



**服务器程序**

生产环境的电脑就只运行服务器程序。

服务器架起来就不能跑别的了。



#### 8.11.2 TCP/IP协议的配置选项

配置选项都有缺省值。可能在某些场合下对性能有所提升，如果要修改配置项，需要：

-   对这个配置项有明确的理解
-   对相关的配置项，记录缺省值，做出修改
-   修改之后要反复测试：是否有提升性能，是否有副作用



**常见的配置选项：**

在`/etc/sysctl.conf`文件中有一些配置项，有的会影响客户端，有的影响服务器。

客户端的端口一般系统分配。

TCP三次握手，第三次握手一般不携带额外数据，但是这个包可以携带额外数据，可以修改`/etc/sysctl.conf`里面的`net.ipv4.tcp_fastopen`可以修改这个设定。



#### 8.11.3 TCP/协议额外注意的一些算法、概念

**滑动窗口**

TCP引入滑动窗口是为了解决高速传输和流量控制问题（限速）。



**Nagle算法**

这个算法会把几个小报文合成一个大的报文再发送。



**Cork算法**

比Nagle算法更加严格，完全禁止发送小报文，除非累计到一定数量或者超过某一时间



**Keep-Alive机制**

用于关闭已经断开的tcp连接，类似于心跳包。



**SO_LINGER选项**

这个选项用来设置：在连接关闭的时候，是否进行正常的四次挥手。



#### 8.11.4 配置最大允许打开的文件句柄数

查看操作系统下允许使用的最大句柄数：

```bash
cat /proc/sys/fs/file-max
184402
```



查看当前 已经分配的/ 分配但没有使用的/文件句柄最大数目

```bash
cat /proc/sys/fs/file-nr
1344	0	184402
```

限制某个用户使用的最大句柄数：

-   修改`/etc/security/limit.conf`文件

    ```
    root soft nofile 60000
    root hard nofile 60000
    ```

    `soft`是软限制，`hard`是硬限制，前者一般小于等于后者。软限制可以在程序中可以修改，硬限制改不了。

    使用`ulimit -n`查看系统允许当前用户进程打开的文件数限制，使用` ulimit -HSn 65535`可以修改硬限制和软限制，但是这是临时设置，只在当前会话有效。永久有效需要修改`/etc/security/limit.conf`文件。



推荐文章：https://blog.csdn.net/xyang81/article/details/52779229



### 8.12 内存池补充说明

为什么没有使用内存池技术，必要性不大。

速度上快不了多少，但是可以节省碎片空间。

TCMALLOC库地址：https://github.com/gperftools/gperftools，在多线程中比`malloc`快不少。





## 9 总结与展望

### 9.1 技术总结

通讯框架+业务逻辑框架

项目包括：

-   完整的多线程高并发服务器程序
-   按照「包头+包体」格式，
-   根据收到的数据包，来执行不同的业务逻辑
-   把业务逻辑产生的结果正确返回给客户端



开发技术：

-   epoll高并发通讯技术，LT模式
-   通过线程池技术处理业务逻辑
-   多线程，线程之间的同步技术包括互斥量、信号量等
-   信号、日志打印、守护进程等



借鉴nginx：

-   一个master进程，多个worker进程的进程架构
-   epoll的一些实现，但是官方使用的是ET模式
-   nginx的接收数据包和发送数据包的核心代码



没有借鉴官方nginx：

-   epoll中的LT模式
-   一套线程池来处理业务逻辑，调用适当的业务逻辑处理函数，直至处理完毕把数据发送回客户端
-   连接池中连接的延迟回收（核心），消除很多导致服务器不稳定的因素
-   关于数据发送逻辑的发送线程



Windows没有epoll，采用的是iocp。

